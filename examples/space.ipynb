{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "20d310c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python path updated:\n",
      "sys.path includes: /home/ubuntu/tensorflow_test/control/real-timeRL/realtime-atari-jax\n",
      "PYTHONPATH env var: /home/ubuntu/tensorflow_test/control/real-timeRL/realtime-atari-jax:\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import time\n",
    "import math\n",
    "import shutil\n",
    "import pickle\n",
    "from functools import partial\n",
    "from typing import NamedTuple, Literal\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import optax\n",
    "from flax import nnx\n",
    "from omegaconf import OmegaConf\n",
    "from pydantic import BaseModel\n",
    "import wandb\n",
    "\n",
    "\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Add the path to sys.path for the current Python session\n",
    "new_path = \"/home/ubuntu/tensorflow_test/control/real-timeRL/realtime-atari-jax\"\n",
    "\n",
    "# Add to sys.path if not already there\n",
    "if new_path not in sys.path:\n",
    "    sys.path.insert(0, new_path)\n",
    "\n",
    "# Also set PYTHONPATH for any subprocesses\n",
    "os.environ[\"PYTHONPATH\"] = f\"{new_path}:{os.environ.get('PYTHONPATH', '')}\"\n",
    "\n",
    "# Verify it worked\n",
    "print(\"Python path updated:\")\n",
    "print(f\"sys.path includes: {new_path}\")\n",
    "print(f\"PYTHONPATH env var: {os.environ['PYTHONPATH']}\")\n",
    "\n",
    "import pgx\n",
    "from pgx.experimental import auto_reset\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Simple Categorical distribution wrapper using JAX built-ins\n",
    "# -----------------------------\n",
    "class Categorical:\n",
    "    def __init__(self, logits):\n",
    "        self.logits = logits\n",
    "\n",
    "    def sample(self, seed):\n",
    "        return jax.random.categorical(seed, self.logits)\n",
    "\n",
    "    def log_prob(self, value):\n",
    "        log_probs = jax.nn.log_softmax(self.logits)\n",
    "        return jnp.take_along_axis(log_probs, value[..., None], axis=-1).squeeze(-1)\n",
    "\n",
    "    def entropy(self):\n",
    "        log_probs = jax.nn.log_softmax(self.logits)\n",
    "        probs = jax.nn.softmax(self.logits)\n",
    "        return -(probs * log_probs).sum(axis=-1)\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Config\n",
    "# -----------------------------\n",
    "class PPOConfig(BaseModel):\n",
    "    env_name: Literal[\n",
    "        \"minatar-breakout\",\n",
    "        \"minatar-freeway\",\n",
    "        \"minatar-space_invaders\",\n",
    "        \"minatar-asterix\",\n",
    "        \"minatar-seaquest\",\n",
    "    ] = \"minatar-space_invaders\"\n",
    "    seed: int = 0\n",
    "    lr: float = 0.0003\n",
    "    num_envs: int = 4096\n",
    "    num_eval_envs: int = 100\n",
    "    num_steps: int = 128\n",
    "    total_timesteps: int = 20_000_000\n",
    "    update_epochs: int = 3\n",
    "    minibatch_size: int = 4096\n",
    "    gamma: float = 0.99\n",
    "    gae_lambda: float = 0.95\n",
    "    clip_eps: float = 0.2\n",
    "    ent_coef: float = 0.01\n",
    "    vf_coef: float = 0.5\n",
    "    max_grad_norm: float = 0.5\n",
    "    wandb_project: str = \"pgx-minatar-ppo\"\n",
    "    save_model: bool = True\n",
    "    out_models_dir: str = \"/home/ubuntu/tensorflow_test/control/real-timeRL/realtime-atari-jax/examples/minatar-ppo/space_models\"\n",
    "\n",
    "    class Config:\n",
    "        extra = \"forbid\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c7c0c9f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"MinAtar/SpaceInvaders: A fork of github.com/kenjyoung/MinAtar\n",
    "\n",
    "The authors of original MinAtar implementation are:\n",
    "    * Kenny Young (kjyoung@ualberta.ca)\n",
    "    * Tian Tian (ttian@ualberta.ca)\n",
    "The original MinAtar implementation is distributed under GNU General Public License v3.0\n",
    "    * https://github.com/kenjyoung/MinAtar/blob/master/License.txt\n",
    "\"\"\"\n",
    "from typing import Literal, Optional\n",
    "\n",
    "import jax\n",
    "import jax.lax as lax\n",
    "from jax import numpy as jnp\n",
    "\n",
    "import pgx.core as core\n",
    "from pgx._src.struct import dataclass\n",
    "from pgx._src.types import Array, PRNGKey\n",
    "\n",
    "FALSE = jnp.bool_(False)\n",
    "TRUE = jnp.bool_(True)\n",
    "\n",
    "SHOT_COOL_DOWN = jnp.int32(5)\n",
    "ENEMY_MOVE_INTERVAL = jnp.int32(12)\n",
    "ENEMY_SHOT_INTERVAL = jnp.int32(10)\n",
    "\n",
    "ZERO = jnp.int32(0)\n",
    "NINE = jnp.int32(9)\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class State(core.State):\n",
    "    current_player: Array = jnp.int32(0)\n",
    "    observation: Array = jnp.zeros((10, 10, 6), dtype=jnp.bool_)\n",
    "    rewards: Array = jnp.zeros(1, dtype=jnp.float32)  # (1,)\n",
    "    terminated: Array = FALSE\n",
    "    truncated: Array = FALSE\n",
    "    legal_action_mask: Array = jnp.ones(4, dtype=jnp.bool_)\n",
    "    _step_count: Array = jnp.int32(0)\n",
    "    # --- MinAtar SpaceInvaders specific ---\n",
    "    _pos: Array = jnp.int32(5)\n",
    "    _f_bullet_map: Array = jnp.zeros((10, 10), dtype=jnp.bool_)\n",
    "    _e_bullet_map: Array = jnp.zeros((10, 10), dtype=jnp.bool_)\n",
    "    _alien_map: Array = (\n",
    "        jnp.zeros((10, 10), dtype=jnp.bool_).at[0:4, 2:8].set(TRUE)\n",
    "    )\n",
    "    _alien_dir: Array = jnp.int32(-1)\n",
    "    _enemy_move_interval: Array = ENEMY_MOVE_INTERVAL\n",
    "    _alien_move_timer: Array = ENEMY_MOVE_INTERVAL\n",
    "    _alien_shot_timer: Array = ENEMY_SHOT_INTERVAL\n",
    "    _ramp_index: Array = jnp.int32(0)\n",
    "    _shot_timer: Array = jnp.int32(0)\n",
    "    _terminal: Array = FALSE\n",
    "    _last_action: Array = jnp.int32(0)\n",
    "\n",
    "    @property\n",
    "    def env_id(self) -> core.EnvId:\n",
    "        return \"minatar-space_invaders\"\n",
    "\n",
    "    def to_svg(\n",
    "        self,\n",
    "        *,\n",
    "        color_theme: Optional[Literal[\"light\", \"dark\"]] = None,\n",
    "        scale: Optional[float] = None,\n",
    "    ) -> str:\n",
    "        del color_theme, scale\n",
    "        from .utils import visualize_minatar\n",
    "\n",
    "        return visualize_minatar(self)\n",
    "\n",
    "    def save_svg(\n",
    "        self,\n",
    "        filename,\n",
    "        *,\n",
    "        color_theme: Optional[Literal[\"light\", \"dark\"]] = None,\n",
    "        scale: Optional[float] = None,\n",
    "    ) -> None:\n",
    "        from .utils import visualize_minatar\n",
    "\n",
    "        visualize_minatar(self, filename)\n",
    "\n",
    "\n",
    "class MinAtarSpaceInvaders(core.Env):\n",
    "    def __init__(\n",
    "        self,\n",
    "        *,\n",
    "        use_minimal_action_set: bool = True,\n",
    "        sticky_action_prob: float = 0.1,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.use_minimal_action_set = use_minimal_action_set\n",
    "        self.sticky_action_prob: float = sticky_action_prob\n",
    "        self.minimal_action_set = jnp.int32([0, 1, 3, 5])\n",
    "        self.legal_action_mask = jnp.ones(6, dtype=jnp.bool_)\n",
    "        if self.use_minimal_action_set:\n",
    "            self.legal_action_mask = jnp.ones(\n",
    "                self.minimal_action_set.shape[0], dtype=jnp.bool_\n",
    "            )\n",
    "\n",
    "    def step(\n",
    "        self, state: core.State, action: Array, key: Optional[Array] = None\n",
    "    ) -> core.State:\n",
    "        assert key is not None, (\n",
    "            \"v2.0.0 changes the signature of step. Please specify PRNGKey at the third argument:\\n\\n\"\n",
    "            \"  * <  v2.0.0: step(state, action)\\n\"\n",
    "            \"  * >= v2.0.0: step(state, action, key)\\n\\n\"\n",
    "            \"See v2.0.0 release note for more details:\\n\\n\"\n",
    "            \"  https://github.com/sotetsuk/pgx/releases/tag/v2.0.0\"\n",
    "        )\n",
    "        return super().step(state, action, key)\n",
    "\n",
    "    def _init(self, key: PRNGKey) -> State:\n",
    "        state = State()\n",
    "        state = state.replace(legal_action_mask=self.legal_action_mask)  # type: ignore\n",
    "        return state  # type: ignore\n",
    "\n",
    "    def _step(self, state: core.State, action, key) -> State:\n",
    "        state = state.replace(legal_action_mask=self.legal_action_mask)  # type: ignore\n",
    "        action = jax.lax.select(\n",
    "            self.use_minimal_action_set,\n",
    "            self.minimal_action_set[action],\n",
    "            action,\n",
    "        )\n",
    "        return _step(state, action, key, self.sticky_action_prob)  # type: ignore\n",
    "\n",
    "    def _observe(self, state: core.State, player_id: Array) -> Array:\n",
    "        assert isinstance(state, State)\n",
    "        return _observe(state)\n",
    "\n",
    "    @property\n",
    "    def id(self) -> core.EnvId:\n",
    "        return \"minatar-space_invaders\"\n",
    "\n",
    "    @property\n",
    "    def version(self) -> str:\n",
    "        return \"v1\"\n",
    "\n",
    "    @property\n",
    "    def num_players(self):\n",
    "        return 1\n",
    "\n",
    "\n",
    "def _step(\n",
    "    state: State,\n",
    "    action: Array,\n",
    "    key,\n",
    "    sticky_action_prob,\n",
    "):\n",
    "    action = jnp.int32(action)\n",
    "    action = jax.lax.cond(\n",
    "        jax.random.uniform(key) < sticky_action_prob,\n",
    "        lambda: state._last_action,\n",
    "        lambda: action,\n",
    "    )\n",
    "    return _step_det(state, action)\n",
    "\n",
    "\n",
    "def _observe(state: State) -> Array:\n",
    "    obs = jnp.zeros((10, 10, 6), dtype=jnp.bool_)\n",
    "    obs = obs.at[9, state._pos, 0].set(TRUE)\n",
    "    obs = obs.at[:, :, 1].set(state._alien_map)\n",
    "    obs = obs.at[:, :, 2].set(\n",
    "        lax.cond(\n",
    "            state._alien_dir < 0,\n",
    "            lambda: state._alien_map,\n",
    "            lambda: jnp.zeros_like(state._alien_map),\n",
    "        )\n",
    "    )\n",
    "    obs = obs.at[:, :, 3].set(\n",
    "        lax.cond(\n",
    "            state._alien_dir < 0,\n",
    "            lambda: jnp.zeros_like(state._alien_map),\n",
    "            lambda: state._alien_map,\n",
    "        )\n",
    "    )\n",
    "    obs = obs.at[:, :, 4].set(state._f_bullet_map)\n",
    "    obs = obs.at[:, :, 5].set(state._e_bullet_map)\n",
    "    return obs\n",
    "\n",
    "\n",
    "def _step_det(\n",
    "    state: State,\n",
    "    action: Array,\n",
    "):\n",
    "    r = jnp.float32(0)\n",
    "\n",
    "    pos = state._pos\n",
    "    f_bullet_map = state._f_bullet_map\n",
    "    e_bullet_map = state._e_bullet_map\n",
    "    alien_map = state._alien_map\n",
    "    alien_dir = state._alien_dir\n",
    "    enemy_move_interval = state._enemy_move_interval\n",
    "    alien_move_timer = state._alien_move_timer\n",
    "    alien_shot_timer = state._alien_shot_timer\n",
    "    ramp_index = state._ramp_index\n",
    "    shot_timer = state._shot_timer\n",
    "    terminal = state._terminal\n",
    "\n",
    "    # Resolve player action\n",
    "    # action_map = ['n','l','u','r','d','f']\n",
    "    pos, f_bullet_map, shot_timer = _resole_action(\n",
    "        pos, f_bullet_map, shot_timer, action\n",
    "    )\n",
    "\n",
    "    # Update Friendly Bullets\n",
    "    f_bullet_map = jnp.roll(f_bullet_map, -1, axis=0)\n",
    "    f_bullet_map = f_bullet_map.at[9, :].set(FALSE)\n",
    "\n",
    "    # Update Enemy Bullets\n",
    "    e_bullet_map = jnp.roll(e_bullet_map, 1, axis=0)\n",
    "    e_bullet_map = e_bullet_map.at[0, :].set(FALSE)\n",
    "    terminal = lax.cond(e_bullet_map[9, pos], lambda: TRUE, lambda: terminal)\n",
    "\n",
    "    # Update aliens\n",
    "    terminal = lax.cond(alien_map[9, pos], lambda: TRUE, lambda: terminal)\n",
    "    alien_move_timer, alien_map, alien_dir, terminal = lax.cond(\n",
    "        alien_move_timer == 0,\n",
    "        lambda: _update_alien_by_move_timer(\n",
    "            alien_map, alien_dir, enemy_move_interval, pos, terminal\n",
    "        ),\n",
    "        lambda: (alien_move_timer, alien_map, alien_dir, terminal),\n",
    "    )\n",
    "    timer_zero = alien_shot_timer == 0\n",
    "    alien_shot_timer = lax.cond(\n",
    "        timer_zero, lambda: ENEMY_SHOT_INTERVAL, lambda: alien_shot_timer\n",
    "    )\n",
    "    e_bullet_map = lax.cond(\n",
    "        timer_zero,\n",
    "        lambda: e_bullet_map.at[_nearest_alien(pos, alien_map)].set(TRUE),\n",
    "        lambda: e_bullet_map,\n",
    "    )\n",
    "\n",
    "    kill_locations = alien_map & (alien_map == f_bullet_map)\n",
    "\n",
    "    r += jnp.sum(kill_locations, dtype=jnp.float32)\n",
    "    alien_map = alien_map & (~kill_locations)\n",
    "    f_bullet_map = f_bullet_map & (~kill_locations)\n",
    "\n",
    "    # Update various timers\n",
    "    shot_timer -= shot_timer > 0\n",
    "    alien_move_timer -= 1\n",
    "    alien_shot_timer -= 1\n",
    "    ramping = True\n",
    "    is_enemy_zero = jnp.count_nonzero(alien_map) == 0\n",
    "    enemy_move_interval, ramp_index = lax.cond(\n",
    "        is_enemy_zero & (enemy_move_interval > 6) & ramping,\n",
    "        lambda: (enemy_move_interval - 1, ramp_index + 1),\n",
    "        lambda: (enemy_move_interval, ramp_index),\n",
    "    )\n",
    "    alien_map = lax.cond(\n",
    "        is_enemy_zero,\n",
    "        lambda: alien_map.at[0:4, 2:8].set(TRUE),\n",
    "        lambda: alien_map,\n",
    "    )\n",
    "\n",
    "    return state.replace(  # type: ignore\n",
    "        _pos=pos,\n",
    "        _f_bullet_map=f_bullet_map,\n",
    "        _e_bullet_map=e_bullet_map,\n",
    "        _alien_map=alien_map,\n",
    "        _alien_dir=alien_dir,\n",
    "        _enemy_move_interval=enemy_move_interval,\n",
    "        _alien_move_timer=alien_move_timer,\n",
    "        _alien_shot_timer=alien_shot_timer,\n",
    "        _ramp_index=ramp_index,\n",
    "        _shot_timer=shot_timer,\n",
    "        _terminal=terminal,\n",
    "        _last_action=action,\n",
    "        rewards=r[jnp.newaxis],\n",
    "        terminated=terminal,\n",
    "    )\n",
    "\n",
    "\n",
    "def _resole_action(pos, f_bullet_map, shot_timer, action):\n",
    "    f_bullet_map = lax.cond(\n",
    "        (action == 5) & (shot_timer == 0),\n",
    "        lambda: f_bullet_map.at[9, pos].set(TRUE),\n",
    "        lambda: f_bullet_map,\n",
    "    )\n",
    "    shot_timer = lax.cond(\n",
    "        (action == 5) & (shot_timer == 0),\n",
    "        lambda: SHOT_COOL_DOWN,\n",
    "        lambda: shot_timer,\n",
    "    )\n",
    "    pos = lax.cond(\n",
    "        action == 1, lambda: jax.lax.max(ZERO, pos - 1), lambda: pos\n",
    "    )\n",
    "    pos = lax.cond(\n",
    "        action == 3, lambda: jax.lax.min(NINE, pos + 1), lambda: pos\n",
    "    )\n",
    "    return pos, f_bullet_map, shot_timer\n",
    "\n",
    "\n",
    "def _nearest_alien(pos, alien_map):\n",
    "    search_order = jnp.argsort(jnp.abs(jnp.arange(10, dtype=jnp.int32) - pos))\n",
    "    ix = lax.while_loop(\n",
    "        lambda i: jnp.sum(alien_map[:, search_order[i]]) <= 0,\n",
    "        lambda i: i + 1,\n",
    "        0,\n",
    "    )\n",
    "    ix = search_order[ix]\n",
    "    j = lax.while_loop(lambda i: alien_map[i, ix] == 0, lambda i: i - 1, 9)\n",
    "    return (j, ix)\n",
    "\n",
    "\n",
    "def _update_alien_by_move_timer(\n",
    "    alien_map, alien_dir, enemy_move_interval, pos, terminal\n",
    "):\n",
    "    alien_move_timer = lax.min(\n",
    "        jnp.sum(alien_map, dtype=jnp.int32), enemy_move_interval\n",
    "    )\n",
    "    cond = ((jnp.sum(alien_map[:, 0]) > 0) & (alien_dir < 0)) | (\n",
    "        (jnp.sum(alien_map[:, 9]) > 0) & (alien_dir > 0)\n",
    "    )\n",
    "    terminal = lax.cond(\n",
    "        cond & (jnp.sum(alien_map[9, :]) > 0),\n",
    "        lambda: jnp.bool_(True),\n",
    "        lambda: terminal,\n",
    "    )\n",
    "    alien_dir = lax.cond(cond, lambda: -alien_dir, lambda: alien_dir)\n",
    "    alien_map = lax.cond(\n",
    "        cond,\n",
    "        lambda: jnp.roll(alien_map, 1, axis=0),\n",
    "        lambda: jnp.roll(alien_map, alien_dir, axis=1),\n",
    "    )\n",
    "    terminal = lax.cond(\n",
    "        alien_map[9, pos], lambda: jnp.bool_(True), lambda: terminal\n",
    "    )\n",
    "    return alien_move_timer, alien_map, alien_dir, terminal\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7738e895",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = PPOConfig(\n",
    "    env_name=\"minatar-space_invaders\",)\n",
    "env = MinAtarSpaceInvaders()\n",
    "num_updates = args.total_timesteps // args.num_envs // args.num_steps\n",
    "num_minibatches = args.num_envs * args.num_steps // args.minibatch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a367aade",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# NNX Actor-Critic\n",
    "# -----------------------------\n",
    "def pool_out_dim(n: int, window: int = 2, stride: int = 2, padding: str = \"VALID\") -> int:\n",
    "    # Matches flax.linen/nnx pooling semantics for VALID padding\n",
    "    if padding.upper() == \"VALID\":\n",
    "        return (n - window) // stride + 1\n",
    "    # Fallback (not used here)\n",
    "    return math.ceil(n / stride)\n",
    "\n",
    "\n",
    "class ActorCritic(nnx.Module):\n",
    "    def __init__(self, num_actions: int, obs_shape, activation: str = \"tanh\", *, rngs: nnx.Rngs):\n",
    "        assert activation in [\"relu\", \"tanh\"]\n",
    "        self.num_actions = num_actions\n",
    "        self.activation = activation\n",
    "\n",
    "        H, W, C = obs_shape  # NHWC expected by flax.nnx.Conv\n",
    "        # Convolution (channels-last). Default padding is 'SAME'.\n",
    "        self.conv = nnx.Conv(in_features=C, out_features=32, kernel_size=(2, 2), rngs=rngs)\n",
    "\n",
    "        # AvgPool params are fixed; keep a partial for clean callsites\n",
    "        self.avg_pool = partial(nnx.avg_pool, window_shape=(2, 2), strides=(2, 2), padding=\"VALID\")\n",
    "\n",
    "        # After conv ('SAME') + avg_pool('VALID', 2x2, stride 2) the spatial dims become:\n",
    "        H2 = pool_out_dim(H, 2, 2, \"VALID\")\n",
    "        W2 = pool_out_dim(W, 2, 2, \"VALID\")\n",
    "        flatten_dim = H2 * W2 * 32\n",
    "\n",
    "        # Shared torso\n",
    "        self.fc = nnx.Linear(flatten_dim, 64, rngs=rngs)\n",
    "\n",
    "        # Actor head: 64 -> 64 -> 64 -> num_actions (two hidden layers like original)\n",
    "        self.actor_h1 = nnx.Linear(64, 64, rngs=rngs)\n",
    "        self.actor_h2 = nnx.Linear(64, 64, rngs=rngs)\n",
    "        self.actor_out = nnx.Linear(64, num_actions, rngs=rngs)\n",
    "\n",
    "        # Critic head: 64 -> 64 -> 64 -> 1 (two hidden layers like original)\n",
    "        self.critic_h1 = nnx.Linear(64, 64, rngs=rngs)\n",
    "        self.critic_h2 = nnx.Linear(64, 64, rngs=rngs)\n",
    "        self.critic_out = nnx.Linear(64, 1, rngs=rngs)\n",
    "\n",
    "    def _act(self, x):\n",
    "        return nnx.relu(x) if self.activation == \"relu\" else nnx.tanh(x)\n",
    "\n",
    "    def __call__(self, x):\n",
    "        x = x.astype(jnp.float32)\n",
    "        x = self.conv(x)\n",
    "        x = nnx.relu(x)\n",
    "        x = self.avg_pool(x)\n",
    "        x = x.reshape((x.shape[0], -1))  # flatten\n",
    "        x = nnx.relu(self.fc(x))\n",
    "\n",
    "        a = self._act(self.actor_h1(x))\n",
    "        a = self._act(self.actor_h2(a))\n",
    "        logits = self.actor_out(a)\n",
    "\n",
    "        v = self._act(self.critic_h1(x))\n",
    "        v = self._act(self.critic_h2(v))\n",
    "        value = self.critic_out(v)\n",
    "\n",
    "        return logits, jnp.squeeze(value, axis=-1)\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Optimizer (Optax via NNX wrapper)\n",
    "# -----------------------------\n",
    "tx = optax.chain(\n",
    "    optax.clip_by_global_norm(args.max_grad_norm),\n",
    "    optax.adam(args.lr, eps=1e-5),\n",
    ")\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Rollout container\n",
    "# -----------------------------\n",
    "class Transition(NamedTuple):\n",
    "    done: jnp.ndarray\n",
    "    action: jnp.ndarray\n",
    "    value: jnp.ndarray\n",
    "    reward: jnp.ndarray\n",
    "    log_prob: jnp.ndarray\n",
    "    obs: jnp.ndarray\n",
    "\n",
    "\n",
    "def save_checkpoint(model: nnx.Module, step: int) -> str:\n",
    "    checkpoint_path = os.path.join(\n",
    "        args.out_models_dir,\n",
    "        f\"{args.env_name}-seed={args.seed}-steps={step}.ckpt\",\n",
    "    )\n",
    "    with open(checkpoint_path, \"wb\") as f:\n",
    "        pickle.dump(nnx.state(model, nnx.Param), f)\n",
    "    return checkpoint_path\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Update step (collect + optimize), jitted with NNX\n",
    "# -----------------------------\n",
    "def make_update_step():\n",
    "    step_fn = jax.vmap(auto_reset(env.step, env.init))\n",
    "\n",
    "    @nnx.jit(donate_argnames=(\"model\", \"optimizer\"))\n",
    "    def _update_step(model: nnx.Module,\n",
    "                     optimizer: nnx.Optimizer,\n",
    "                     env_state,\n",
    "                     last_obs,\n",
    "                     rng):\n",
    "        # -------- Collect trajectories --------\n",
    "        def _env_step(runner_state, _):\n",
    "            model, optimizer, env_state, last_obs, rng = runner_state\n",
    "\n",
    "            # Policy\n",
    "            rng, _rng = jax.random.split(rng)\n",
    "            logits, value = model(last_obs)\n",
    "            pi = Categorical(logits=logits)\n",
    "            action = pi.sample(seed=_rng)\n",
    "            log_prob = pi.log_prob(action)\n",
    "\n",
    "            # Env step\n",
    "            rng, _rng = jax.random.split(rng)\n",
    "            keys = jax.random.split(_rng, env_state.observation.shape[0])\n",
    "            env_state = step_fn(env_state, action, keys)\n",
    "\n",
    "            transition = Transition(\n",
    "                env_state.terminated,\n",
    "                action,\n",
    "                value,\n",
    "                jnp.squeeze(env_state.rewards),\n",
    "                log_prob,\n",
    "                last_obs,\n",
    "            )\n",
    "            runner_state = (model, optimizer, env_state, env_state.observation, rng)\n",
    "            return runner_state, transition\n",
    "\n",
    "        runner_state = (model, optimizer, env_state, last_obs, rng)\n",
    "        runner_state, traj_batch = jax.lax.scan(_env_step, runner_state, None, length=args.num_steps)\n",
    "\n",
    "        # -------- Advantage / targets (GAE) --------\n",
    "        model, optimizer, env_state, last_obs, rng = runner_state\n",
    "        _, last_val = model(last_obs)\n",
    "\n",
    "        def _get_advantages(gae_and_next_value, transition):\n",
    "            gae, next_value = gae_and_next_value\n",
    "            done, value, reward = transition.done, transition.value, transition.reward\n",
    "            delta = reward + args.gamma * next_value * (1 - done) - value\n",
    "            gae = delta + args.gamma * args.gae_lambda * (1 - done) * gae\n",
    "            return (gae, value), gae\n",
    "\n",
    "        (_, _), advantages = jax.lax.scan(\n",
    "            _get_advantages,\n",
    "            (jnp.zeros_like(last_val), last_val),\n",
    "            traj_batch,\n",
    "            reverse=True,\n",
    "            unroll=16,\n",
    "        )\n",
    "        targets = advantages + traj_batch.value\n",
    "\n",
    "        # -------- SGD epochs --------\n",
    "        def _update_epoch(update_state, _):\n",
    "            model, optimizer, traj_batch, advantages, targets, rng = update_state\n",
    "\n",
    "            def _update_minibatch(state, minibatch):\n",
    "                model, optimizer = state\n",
    "                mb_traj, mb_adv, mb_targets = minibatch\n",
    "\n",
    "                def _loss_fn(model: nnx.Module, traj: Transition, gae, targets):\n",
    "                    # Re-run policy\n",
    "                    logits, value = model(traj.obs)\n",
    "                    pi = Categorical(logits=logits)\n",
    "                    log_prob = pi.log_prob(traj.action)\n",
    "\n",
    "                    # Value loss (clipped)\n",
    "                    value_pred_clipped = traj.value + (value - traj.value).clip(-args.clip_eps, args.clip_eps)\n",
    "                    v_loss_unclipped = jnp.square(value - targets)\n",
    "                    v_loss_clipped = jnp.square(value_pred_clipped - targets)\n",
    "                    value_loss = 0.5 * jnp.maximum(v_loss_unclipped, v_loss_clipped).mean()\n",
    "\n",
    "                    # Policy loss (clipped)\n",
    "                    ratio = jnp.exp(log_prob - traj.log_prob)\n",
    "                    gae = (gae - gae.mean()) / (gae.std() + 1e-8)\n",
    "                    loss_actor1 = ratio * gae\n",
    "                    loss_actor2 = jnp.clip(ratio, 1.0 - args.clip_eps, 1.0 + args.clip_eps) * gae\n",
    "                    loss_actor = -jnp.minimum(loss_actor1, loss_actor2).mean()\n",
    "\n",
    "                    # Entropy bonus\n",
    "                    entropy = pi.entropy().mean()\n",
    "\n",
    "                    total = loss_actor + args.vf_coef * value_loss - args.ent_coef * entropy\n",
    "                    return total, (value_loss, loss_actor, entropy)\n",
    "\n",
    "                # Compute grads w.r.t. model Params\n",
    "                (total_loss, aux), grads = nnx.value_and_grad(\n",
    "                    _loss_fn, has_aux=True, argnums=nnx.DiffState(0, nnx.Param)\n",
    "                )(model, mb_traj, mb_adv, mb_targets)\n",
    "\n",
    "                # Optax step via NNX Optimizer (updates model in-place)\n",
    "                optimizer.update(model, grads)\n",
    "\n",
    "                return (model, optimizer), (total_loss, aux)\n",
    "\n",
    "            # Shuffle + minibatch\n",
    "            rng, _rng = jax.random.split(rng)\n",
    "            batch_size = args.minibatch_size * num_minibatches\n",
    "            assert batch_size == args.num_steps * args.num_envs, \"batch size must equal steps * envs\"\n",
    "\n",
    "            batch = (traj_batch, advantages, targets)\n",
    "            batch = jax.tree.map(lambda x: x.reshape((batch_size,) + x.shape[2:]), batch)\n",
    "            permutation = jax.random.permutation(_rng, batch_size)\n",
    "            shuffled = jax.tree.map(lambda x: jnp.take(x, permutation, axis=0), batch)\n",
    "            minibatches = jax.tree.map(\n",
    "                lambda x: jnp.reshape(x, [num_minibatches, -1] + list(x.shape[1:])),\n",
    "                shuffled,\n",
    "            )\n",
    "\n",
    "            (model, optimizer), losses = jax.lax.scan(_update_minibatch, (model, optimizer), minibatches)\n",
    "            update_state = (model, optimizer, traj_batch, advantages, targets, rng)\n",
    "            return update_state, losses\n",
    "\n",
    "        update_state = (model, optimizer, traj_batch, advantages, targets, rng)\n",
    "        update_state, loss_info = jax.lax.scan(_update_epoch, update_state, None, length=args.update_epochs)\n",
    "\n",
    "        model, optimizer, _, _, _, rng = update_state\n",
    "        runner_state = (model, optimizer, env_state, last_obs, rng)\n",
    "        return runner_state, loss_info\n",
    "\n",
    "    return _update_step\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Evaluation (greedy sample)\n",
    "# -----------------------------\n",
    "@nnx.jit\n",
    "def evaluate(model: nnx.Module, rng_key):\n",
    "    step_fn = jax.vmap(env.step)\n",
    "    rng_key, sub_key = jax.random.split(rng_key)\n",
    "    subkeys = jax.random.split(sub_key, args.num_eval_envs)\n",
    "    state = jax.vmap(env.init)(subkeys)\n",
    "    R = jnp.zeros_like(state.rewards)\n",
    "\n",
    "    def cond_fn(tup):\n",
    "        state, _, _ = tup\n",
    "        return ~state.terminated.all()\n",
    "\n",
    "    def loop_fn(tup):\n",
    "        state, R, rng_key = tup\n",
    "        logits, _value = model(state.observation)\n",
    "        pi = Categorical(logits=logits)\n",
    "        rng_key, _rng = jax.random.split(rng_key)\n",
    "        action = pi.sample(seed=_rng)\n",
    "        rng_key, _rng = jax.random.split(rng_key)\n",
    "        keys = jax.random.split(_rng, state.observation.shape[0])\n",
    "        state = step_fn(state, action, keys)\n",
    "        return state, R + state.rewards, rng_key\n",
    "\n",
    "    state, R, _ = jax.lax.while_loop(cond_fn, loop_fn, (state, R, rng_key))\n",
    "    return R.mean()\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Training Loop\n",
    "# -----------------------------\n",
    "def train(rng):\n",
    "    tt = 0.0\n",
    "    st = time.time()\n",
    "\n",
    "    # Model + optimizer\n",
    "    rng, _rng = jax.random.split(rng)\n",
    "    obs_shape = env.observation_shape\n",
    "    model = ActorCritic(env.num_actions, obs_shape=obs_shape, activation=\"tanh\", rngs=nnx.Rngs(_rng))\n",
    "    optimizer = nnx.Optimizer(model, tx, wrt=nnx.Param)\n",
    "\n",
    "    # Update function\n",
    "    update_step = make_update_step()\n",
    "\n",
    "    # Init envs\n",
    "    rng, _rng = jax.random.split(rng)\n",
    "    reset_rng = jax.random.split(_rng, args.num_envs)\n",
    "    env_state = jax.jit(jax.vmap(env.init))(reset_rng)\n",
    "    last_obs = env_state.observation\n",
    "\n",
    "    rng, _rng = jax.random.split(rng)\n",
    "    runner_state = (model, optimizer, env_state, last_obs, _rng)\n",
    "\n",
    "    # Warmup (compile)\n",
    "    _, _ = update_step(*runner_state)\n",
    "\n",
    "    # initial evaluation\n",
    "    et = time.time()\n",
    "    tt += et - st\n",
    "    rng, _rng = jax.random.split(rng)\n",
    "    eval_R = evaluate(runner_state[0], _rng)\n",
    "    steps = 0\n",
    "    training_total = args.num_envs * args.num_steps * num_updates\n",
    "    checkpoint_targets = []\n",
    "    checkpoint_paths = {}\n",
    "    if args.save_model:\n",
    "        os.makedirs(args.out_models_dir, exist_ok=True)\n",
    "        base_interval = max(1, math.ceil(training_total / 4))\n",
    "        checkpoint_targets = [min(training_total, base_interval * i) for i in range(1, 4)]\n",
    "        checkpoint_targets.append(training_total)\n",
    "        checkpoint_targets = sorted(set(checkpoint_targets))\n",
    "    log = {\"sec\": tt, f\"{args.env_name}/eval_R\": float(eval_R), \"steps\": steps}\n",
    "    print(log)\n",
    "    wandb.log(log)\n",
    "    st = time.time()\n",
    "\n",
    "    for _ in range(num_updates):\n",
    "        runner_state, loss_info = update_step(*runner_state)\n",
    "        model, optimizer, env_state, last_obs, rng = runner_state\n",
    "        steps += args.num_envs * args.num_steps\n",
    "\n",
    "        # evaluation\n",
    "        et = time.time()\n",
    "        tt += et - st\n",
    "        rng, _rng = jax.random.split(rng)\n",
    "        eval_R = evaluate(model, _rng)\n",
    "        log = {\"sec\": tt, f\"{args.env_name}/eval_R\": float(eval_R), \"steps\": steps}\n",
    "        print(log)\n",
    "        wandb.log(log)\n",
    "        st = time.time()\n",
    "        if args.save_model:\n",
    "            for target in checkpoint_targets:\n",
    "                if steps >= target and target not in checkpoint_paths:\n",
    "                    checkpoint_paths[target] = save_checkpoint(model, target)\n",
    "\n",
    "    if args.save_model:\n",
    "        for target in checkpoint_targets:\n",
    "            if steps >= target and target not in checkpoint_paths:\n",
    "                checkpoint_paths[target] = save_checkpoint(runner_state[0], target)\n",
    "\n",
    "    return runner_state, checkpoint_paths  # (model, optimizer, env_state, last_obs, rng)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ca3bcb7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1544095/2311977280.py:1: PydanticDeprecatedSince20: The `dict` method is deprecated; use `model_dump` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.11/migration/\n",
      "  wandb.init(project=args.wandb_project, config=args.dict())\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.21.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/ubuntu/tensorflow_test/control/real-timeRL/realtime-atari-jax/examples/wandb/run-20251108_205444-bu3yus5m</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/aneeshmuppidi19/pgx-minatar-ppo/runs/bu3yus5m' target=\"_blank\">upbeat-eon-27</a></strong> to <a href='https://wandb.ai/aneeshmuppidi19/pgx-minatar-ppo' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/aneeshmuppidi19/pgx-minatar-ppo' target=\"_blank\">https://wandb.ai/aneeshmuppidi19/pgx-minatar-ppo</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/aneeshmuppidi19/pgx-minatar-ppo/runs/bu3yus5m' target=\"_blank\">https://wandb.ai/aneeshmuppidi19/pgx-minatar-ppo/runs/bu3yus5m</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'sec': 7.559342622756958, 'minatar-space_invaders/eval_R': 4.269999980926514, 'steps': 0}\n",
      "{'sec': 7.819483041763306, 'minatar-space_invaders/eval_R': 4.630000114440918, 'steps': 524288}\n",
      "{'sec': 8.08708643913269, 'minatar-space_invaders/eval_R': 6.119999885559082, 'steps': 1048576}\n",
      "{'sec': 8.360270023345947, 'minatar-space_invaders/eval_R': 7.869999885559082, 'steps': 1572864}\n",
      "{'sec': 8.639054775238037, 'minatar-space_invaders/eval_R': 12.029999732971191, 'steps': 2097152}\n",
      "{'sec': 8.918978691101074, 'minatar-space_invaders/eval_R': 13.039999961853027, 'steps': 2621440}\n",
      "{'sec': 9.198667764663696, 'minatar-space_invaders/eval_R': 17.75, 'steps': 3145728}\n",
      "{'sec': 9.479210376739502, 'minatar-space_invaders/eval_R': 19.850000381469727, 'steps': 3670016}\n",
      "{'sec': 9.759523391723633, 'minatar-space_invaders/eval_R': 23.94999885559082, 'steps': 4194304}\n",
      "{'sec': 10.039384603500366, 'minatar-space_invaders/eval_R': 27.44999885559082, 'steps': 4718592}\n",
      "{'sec': 10.31794261932373, 'minatar-space_invaders/eval_R': 28.809999465942383, 'steps': 5242880}\n",
      "{'sec': 10.600192785263062, 'minatar-space_invaders/eval_R': 28.809999465942383, 'steps': 5767168}\n",
      "{'sec': 10.878530502319336, 'minatar-space_invaders/eval_R': 36.189998626708984, 'steps': 6291456}\n",
      "{'sec': 11.15737509727478, 'minatar-space_invaders/eval_R': 38.34000015258789, 'steps': 6815744}\n",
      "{'sec': 11.43694806098938, 'minatar-space_invaders/eval_R': 44.47999954223633, 'steps': 7340032}\n",
      "{'sec': 11.715904712677002, 'minatar-space_invaders/eval_R': 40.54999923706055, 'steps': 7864320}\n",
      "{'sec': 11.99629521369934, 'minatar-space_invaders/eval_R': 42.75, 'steps': 8388608}\n",
      "{'sec': 12.276398420333862, 'minatar-space_invaders/eval_R': 47.65999984741211, 'steps': 8912896}\n",
      "{'sec': 12.555979251861572, 'minatar-space_invaders/eval_R': 46.18000030517578, 'steps': 9437184}\n",
      "{'sec': 12.834313869476318, 'minatar-space_invaders/eval_R': 56.03999710083008, 'steps': 9961472}\n",
      "{'sec': 13.117372274398804, 'minatar-space_invaders/eval_R': 56.2599983215332, 'steps': 10485760}\n",
      "{'sec': 13.395657777786255, 'minatar-space_invaders/eval_R': 57.25, 'steps': 11010048}\n",
      "{'sec': 13.674180746078491, 'minatar-space_invaders/eval_R': 71.43999481201172, 'steps': 11534336}\n",
      "{'sec': 13.952954530715942, 'minatar-space_invaders/eval_R': 70.93999481201172, 'steps': 12058624}\n",
      "{'sec': 14.232018232345581, 'minatar-space_invaders/eval_R': 93.04000091552734, 'steps': 12582912}\n",
      "{'sec': 14.510261297225952, 'minatar-space_invaders/eval_R': 89.72000122070312, 'steps': 13107200}\n",
      "{'sec': 14.789889812469482, 'minatar-space_invaders/eval_R': 89.72000122070312, 'steps': 13631488}\n",
      "{'sec': 15.068217039108276, 'minatar-space_invaders/eval_R': 95.25, 'steps': 14155776}\n",
      "{'sec': 15.346951246261597, 'minatar-space_invaders/eval_R': 116.5199966430664, 'steps': 14680064}\n",
      "{'sec': 15.62747836112976, 'minatar-space_invaders/eval_R': 128.45999145507812, 'steps': 15204352}\n",
      "{'sec': 15.909142017364502, 'minatar-space_invaders/eval_R': 128.08999633789062, 'steps': 15728640}\n",
      "{'sec': 16.187545776367188, 'minatar-space_invaders/eval_R': 126.5999984741211, 'steps': 16252928}\n",
      "{'sec': 16.466368675231934, 'minatar-space_invaders/eval_R': 131.50999450683594, 'steps': 16777216}\n",
      "{'sec': 16.74322009086609, 'minatar-space_invaders/eval_R': 124.11000061035156, 'steps': 17301504}\n",
      "{'sec': 17.021528959274292, 'minatar-space_invaders/eval_R': 152.66000366210938, 'steps': 17825792}\n",
      "{'sec': 17.299262046813965, 'minatar-space_invaders/eval_R': 169.64999389648438, 'steps': 18350080}\n",
      "{'sec': 17.57837462425232, 'minatar-space_invaders/eval_R': 169.77999877929688, 'steps': 18874368}\n",
      "{'sec': 17.856359481811523, 'minatar-space_invaders/eval_R': 169.00999450683594, 'steps': 19398656}\n",
      "{'sec': 18.13342261314392, 'minatar-space_invaders/eval_R': 179.70999145507812, 'steps': 19922944}\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>minatar-space_invaders/eval_R</td><td>▁▁▁▁▁▁▂▂▂▂▂▂▂▂▃▂▃▃▃▃▃▃▄▄▅▄▄▅▅▆▆▆▆▆▇████</td></tr><tr><td>sec</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>steps</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>minatar-space_invaders/eval_R</td><td>179.70999</td></tr><tr><td>sec</td><td>18.13342</td></tr><tr><td>steps</td><td>19922944</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">upbeat-eon-27</strong> at: <a href='https://wandb.ai/aneeshmuppidi19/pgx-minatar-ppo/runs/bu3yus5m' target=\"_blank\">https://wandb.ai/aneeshmuppidi19/pgx-minatar-ppo/runs/bu3yus5m</a><br> View project at: <a href='https://wandb.ai/aneeshmuppidi19/pgx-minatar-ppo' target=\"_blank\">https://wandb.ai/aneeshmuppidi19/pgx-minatar-ppo</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20251108_205444-bu3yus5m/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.init(project=args.wandb_project, config=args.dict())\n",
    "rng = jax.random.PRNGKey(args.seed)\n",
    "out, checkpoint_paths = train(rng)\n",
    "if args.save_model:\n",
    "    model = out[0]\n",
    "    if checkpoint_paths:\n",
    "        final_step = max(checkpoint_paths)\n",
    "        shutil.copyfile(\n",
    "            checkpoint_paths[final_step],\n",
    "            os.path.join(\n",
    "                args.out_models_dir,\n",
    "                f\"{args.env_name}-seed={args.seed}.ckpt\",\n",
    "            ),\n",
    "        )\n",
    "    else:\n",
    "        # Save only learnable parameters (nnx.Param state) like Haiku params\n",
    "        os.makedirs(args.out_models_dir, exist_ok=True)\n",
    "        with open(\n",
    "            os.path.join(\n",
    "                args.out_models_dir,\n",
    "                f\"{args.env_name}-seed={args.seed}.ckpt\",\n",
    "            ),\n",
    "            \"wb\",\n",
    "        ) as f:\n",
    "            pickle.dump(nnx.state(model, nnx.Param), f)\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df831bbc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
