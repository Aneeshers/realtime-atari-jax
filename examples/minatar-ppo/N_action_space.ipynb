{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "149644b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python path updated:\n",
      "sys.path includes: /home/ubuntu/tensorflow_test/control/real-timeRL/realtime-atari-jax\n",
      "PYTHONPATH env var: /home/ubuntu/tensorflow_test/control/real-timeRL/realtime-atari-jax:\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Add the path to sys.path for the current Python session\n",
    "new_path = \"/home/ubuntu/tensorflow_test/control/real-timeRL/realtime-atari-jax\"\n",
    "\n",
    "# Add to sys.path if not already there\n",
    "if new_path not in sys.path:\n",
    "    sys.path.insert(0, new_path)\n",
    "\n",
    "# Also set PYTHONPATH for any subprocesses\n",
    "os.environ[\"PYTHONPATH\"] = f\"{new_path}:{os.environ.get('PYTHONPATH', '')}\"\n",
    "\n",
    "# Verify it worked\n",
    "print(\"Python path updated:\")\n",
    "print(f\"sys.path includes: {new_path}\")\n",
    "print(f\"PYTHONPATH env var: {os.environ['PYTHONPATH']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ede5aebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import math\n",
    "\n",
    "import jax.numpy as jnp\n",
    "\n",
    "\n",
    "def get_sizes(state):\n",
    "    try:\n",
    "        size = len(state.current_player)\n",
    "        width = math.ceil(math.sqrt(size - 0.1))\n",
    "        if size - (width - 1) ** 2 >= width:\n",
    "            height = width\n",
    "        else:\n",
    "            height = width - 1\n",
    "    except TypeError:\n",
    "        size = 1\n",
    "        width = 1\n",
    "        height = 1\n",
    "    return size, width, height\n",
    "\n",
    "\n",
    "def get_cmap(n_channels):\n",
    "    # import seaborn as sns  # type: ignore\n",
    "    # return cmap = sns.color_palette(\"cubehelix\", n_channels)\n",
    "    assert n_channels in (4, 6, 7, 10)\n",
    "    if n_channels == 4:\n",
    "        return [(0.08605633600581405, 0.23824692404212, 0.30561236308077167), (0.32927729263408284, 0.4762845556584382, 0.1837155549758328), (0.8146245329198283, 0.49548316572322215, 0.5752525936416857), (0.7587183008012618, 0.7922069335474338, 0.9543861221913403)]\n",
    "    elif n_channels == 6:\n",
    "        return [(0.10231025194333628, 0.13952898866828906, 0.2560120319409181), (0.10594361078604106, 0.3809739011595331, 0.27015111282899046), (0.4106130272672762, 0.48044780541672255, 0.1891154277778484), (0.7829183382530567, 0.48158303462490826, 0.48672451968362596), (0.8046168329276406, 0.6365733569301846, 0.8796578402926125), (0.7775608374378459, 0.8840392521212448, 0.9452007992345052)]\n",
    "    elif n_channels == 7:\n",
    "        return [(0.10419418740482515, 0.11632019220053316, 0.2327552016195138), (0.08523511613408935, 0.32661779003565533, 0.2973201282529313), (0.26538761550634205, 0.4675654910052002, 0.1908220644759285), (0.6328422475018423, 0.4747981096220677, 0.29070209208025455), (0.8306875710682655, 0.5175161303658079, 0.6628221028832032), (0.7779565181455343, 0.7069421942599752, 0.9314406084043191), (0.7964528047840354, 0.908668973545918, 0.9398253500983916)]\n",
    "    elif n_channels == 10:\n",
    "        return [(0.09854228363950114, 0.07115215572295082, 0.16957891809124037), (0.09159726558869188, 0.20394337960213008, 0.29623965888210324), (0.09406611799930162, 0.3578871412608098, 0.2837709711722866), (0.23627685553553793, 0.46114369021199075, 0.19770731888985724), (0.49498740849493095, 0.4799034869159042, 0.21147789468974837), (0.7354526513473981, 0.4748861903571046, 0.40254094042448907), (0.8325928529853291, 0.5253446757844744, 0.6869376931865354), (0.7936920632275369, 0.6641337211433709, 0.9042311843062529), (0.7588424692372241, 0.8253990353420474, 0.9542699331220588), (0.8385645211683802, 0.9411869386771845, 0.9357655639413166)]\n",
    "\n",
    "\n",
    "# /home/ubuntu/tensorflow_test/control/real-timeRL/realtime-atari-jax/pgx/minatar/utils.py\n",
    "\n",
    "def visualize_minatar(state, savefile=None, fmt=\"svg\", dpi=160):\n",
    "    # Modified from https://github.com/kenjyoung/MinAtar\n",
    "    try:\n",
    "        import matplotlib.colors as colors  # type: ignore\n",
    "        import matplotlib.pyplot as plt  # type: ignore\n",
    "    except ImportError:\n",
    "        sys.stderr.write(\"MinAtar environment requires matplotlib for visualization. Please install matplotlib.\")\n",
    "        sys.exit(1)\n",
    "\n",
    "    obs = state.observation\n",
    "    n_channels = obs.shape[-1]\n",
    "    cmap = get_cmap(n_channels)\n",
    "    cmap.insert(0, (0, 0, 0))\n",
    "    cmap = colors.ListedColormap(cmap)\n",
    "    bounds = [i for i in range(n_channels + 2)]\n",
    "    norm = colors.BoundaryNorm(bounds, n_channels + 1)\n",
    "    size, w, h = get_sizes(state)\n",
    "    fig, ax = plt.subplots(h, w)\n",
    "    n_channels = obs.shape[-1]\n",
    "    if size == 1:\n",
    "        numerical_state = (\n",
    "            jnp.amax(\n",
    "                obs * jnp.reshape(jnp.arange(n_channels) + 1, (1, 1, -1)), 2\n",
    "            )\n",
    "            + 0.5\n",
    "        )\n",
    "        ax.imshow(numerical_state, cmap=cmap, norm=norm, interpolation=\"none\")\n",
    "        ax.set_axis_off()\n",
    "    else:\n",
    "        for j in range(size):\n",
    "            numerical_state = (\n",
    "                jnp.amax(\n",
    "                    obs[j] * jnp.reshape(jnp.arange(n_channels) + 1, (1, 1, -1)),\n",
    "                    2,\n",
    "                )\n",
    "                + 0.5\n",
    "            )\n",
    "            if h == 1:\n",
    "                ax[j].imshow(numerical_state, cmap=cmap, norm=norm, interpolation=\"none\")\n",
    "                ax[j].set_axis_off()\n",
    "            else:\n",
    "                ax[j // w, j % w].imshow(numerical_state, cmap=cmap, norm=norm, interpolation=\"none\")\n",
    "                ax[j // w, j % w].set_axis_off()\n",
    "\n",
    "    if savefile is None:\n",
    "        # Return in-memory image\n",
    "        if fmt == \"svg\":\n",
    "            from io import StringIO\n",
    "            sio = StringIO()\n",
    "            plt.savefig(sio, format=\"svg\", bbox_inches=\"tight\")\n",
    "            plt.close(fig)\n",
    "            return sio.getvalue()  # str (SVG markup)\n",
    "        else:\n",
    "            from io import BytesIO\n",
    "            bio = BytesIO()\n",
    "            plt.savefig(bio, format=fmt, bbox_inches=\"tight\", dpi=dpi)\n",
    "            plt.close(fig)\n",
    "            bio.seek(0)\n",
    "            return bio.getvalue()  # bytes (e.g., PNG)\n",
    "    else:\n",
    "        plt.savefig(savefile, format=fmt, bbox_inches=\"tight\", dpi=(None if fmt == \"svg\" else dpi))\n",
    "        plt.close(fig)\n",
    "        return savefile\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ac034457",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "MinAtar/Freeway with JIT-compatible K-frame skipping.\n",
    "\n",
    "Changes vs. baseline:\n",
    "- Add `frame_skip: int = 2` to MinAtarFreeway.__init__.\n",
    "- Implement frame skipping inside `_step` via `jax.lax.fori_loop`.\n",
    "- Compute one sticky-processed effective action per external step and repeat it\n",
    "  for `frame_skip` internal steps, accumulating rewards. Each micro-step samples\n",
    "  new car speeds/directions (as in the original logic).\n",
    "\"\"\"\n",
    "\n",
    "from typing import Literal, Optional\n",
    "\n",
    "import jax\n",
    "from jax import numpy as jnp\n",
    "\n",
    "import pgx.core as core\n",
    "from pgx._src.struct import dataclass\n",
    "from pgx._src.types import Array, PRNGKey\n",
    "\n",
    "player_speed = jnp.array(3, dtype=jnp.int32)\n",
    "time_limit = jnp.array(2500, dtype=jnp.int32)\n",
    "\n",
    "FALSE = jnp.bool_(False)\n",
    "TRUE = jnp.bool_(True)\n",
    "ZERO = jnp.array(0, dtype=jnp.int32)\n",
    "ONE = jnp.array(1, dtype=jnp.int32)\n",
    "NINE = jnp.array(9, dtype=jnp.int32)\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class State(core.State):\n",
    "    current_player: Array = jnp.int32(0)\n",
    "    observation: Array = jnp.zeros((10, 10, 7), dtype=jnp.bool_)\n",
    "    rewards: Array = jnp.zeros(1, dtype=jnp.float32)  # (1,)\n",
    "    terminated: Array = FALSE\n",
    "    truncated: Array = FALSE\n",
    "    legal_action_mask: Array = jnp.ones(3, dtype=jnp.bool_)\n",
    "    _step_count: Array = jnp.int32(0)\n",
    "    # --- MinAtar Freeway specific ---\n",
    "    _cars: Array = jnp.zeros((8, 4), dtype=jnp.int32)\n",
    "    _pos: Array = jnp.array(9, dtype=jnp.int32)\n",
    "    _move_timer: Array = jnp.array(player_speed, dtype=jnp.int32)\n",
    "    _terminate_timer: Array = jnp.array(time_limit, dtype=jnp.int32)\n",
    "    _terminal: Array = jnp.array(False, dtype=jnp.bool_)\n",
    "    _last_action: Array = jnp.array(0, dtype=jnp.int32)\n",
    "\n",
    "    @property\n",
    "    def env_id(self) -> core.EnvId:\n",
    "        return \"minatar-freeway\"\n",
    "\n",
    "    def to_svg(\n",
    "        self,\n",
    "        *,\n",
    "        color_theme: Optional[Literal[\"light\", \"dark\"]] = None,\n",
    "        scale: Optional[float] = None,\n",
    "    ) -> str:\n",
    "        del color_theme, scale\n",
    "        from .utils import visualize_minatar\n",
    "\n",
    "        return visualize_minatar(self)\n",
    "\n",
    "    def save_svg(\n",
    "        self,\n",
    "        filename,\n",
    "        *,\n",
    "        color_theme: Optional[Literal[\"light\", \"dark\"]] = None,\n",
    "        scale: Optional[float] = None,\n",
    "    ) -> None:\n",
    "        from .utils import visualize_minatar\n",
    "\n",
    "        visualize_minatar(self, filename)\n",
    "\n",
    "\n",
    "class MinAtarFreeway(core.Env):\n",
    "    def __init__(\n",
    "        self,\n",
    "        *,\n",
    "        use_minimal_action_set: bool = True,\n",
    "        sticky_action_prob: float = 0.1,\n",
    "        frame_skip: int = 2,  # NEW: K-frame skipping (default 2)\n",
    "    ):\n",
    "        super().__init__()\n",
    "        assert frame_skip >= 1, \"frame_skip must be >= 1\"\n",
    "        self.use_minimal_action_set = use_minimal_action_set\n",
    "        self.sticky_action_prob: float = float(sticky_action_prob)\n",
    "        self.frame_skip: int = int(frame_skip)\n",
    "\n",
    "        self.minimal_action_set = jnp.int32([0, 2, 4])\n",
    "        self.legal_action_mask = jnp.ones(6, dtype=jnp.bool_)\n",
    "        if self.use_minimal_action_set:\n",
    "            self.legal_action_mask = jnp.ones(\n",
    "                self.minimal_action_set.shape[0], dtype=jnp.bool_\n",
    "            )\n",
    "\n",
    "    def step(\n",
    "        self, state: core.State, action: Array, key: Optional[Array] = None\n",
    "    ) -> core.State:\n",
    "        assert key is not None, (\n",
    "            \"v2.0.0 changes the signature of step. Please specify PRNGKey at the third argument:\\n\\n\"\n",
    "            \"  * <  v2.0.0: step(state, action)\\n\"\n",
    "            \"  * >= v2.0.0: step(state, action, key)\\n\\n\"\n",
    "            \"See v2.0.0 release note for more details:\\n\\n\"\n",
    "            \"  https://github.com/sotetsuk/pgx/releases/tag/v2.0.0\"\n",
    "        )\n",
    "        return super().step(state, action, key)\n",
    "\n",
    "    def _init(self, key: PRNGKey) -> State:\n",
    "        state = _init(rng=key)  # type: ignore\n",
    "        state = state.replace(legal_action_mask=self.legal_action_mask)  # type: ignore\n",
    "        return state  # type: ignore\n",
    "\n",
    "    def _step(self, state: core.State, action, key) -> State:\n",
    "        assert isinstance(state, State)\n",
    "        # Keep mask current\n",
    "        state = state.replace(legal_action_mask=self.legal_action_mask)  # type: ignore\n",
    "\n",
    "        # Map minimal action set if enabled\n",
    "        action = jax.lax.select(\n",
    "            jnp.bool_(self.use_minimal_action_set),\n",
    "            self.minimal_action_set[action],\n",
    "            action,\n",
    "        )\n",
    "        action = jnp.int32(action)\n",
    "\n",
    "        # Sticky override ONCE per external step; then repeat effective action\n",
    "        key_sticky, key_loop = jax.random.split(key, 2)\n",
    "        effective_action = jax.lax.cond(\n",
    "            jax.random.uniform(key_sticky) < self.sticky_action_prob,\n",
    "            lambda: state._last_action,\n",
    "            lambda: action,\n",
    "        )\n",
    "        effective_action = jnp.int32(effective_action)\n",
    "\n",
    "        # fori_loop carry: (State, total_reward(float32), done(bool), rng)\n",
    "        def body_fn(i, carry):\n",
    "            s, rsum, done, rng = carry\n",
    "\n",
    "            def do_step(args):\n",
    "                s_inner, rsum_inner, _done_inner, rng_inner = args\n",
    "                rng_inner, sub = jax.random.split(rng_inner)\n",
    "                speeds, directions = _random_speed_directions(sub)\n",
    "                s_next = _step_det(state=s_inner, action=effective_action, speeds=speeds, directions=directions)\n",
    "                rsum_next = rsum_inner + s_next.rewards[0]\n",
    "                done_next = s_next.terminated\n",
    "                return (s_next, rsum_next, done_next, rng_inner)\n",
    "\n",
    "            return jax.lax.cond(done, lambda x: x, do_step, (s, rsum, done, rng))\n",
    "\n",
    "        r0 = jnp.array(0.0, dtype=jnp.float32)\n",
    "        d0 = FALSE\n",
    "        state, total_r, _, _ = jax.lax.fori_loop(\n",
    "            0, int(self.frame_skip), body_fn, (state, r0, d0, key_loop)\n",
    "        )\n",
    "\n",
    "        # Overwrite reward with accumulated total for this external step\n",
    "        state = state.replace(rewards=total_r[jnp.newaxis])  # type: ignore\n",
    "        return state  # type: ignore\n",
    "\n",
    "    def _observe(self, state: core.State, player_id: Array) -> Array:\n",
    "        assert isinstance(state, State)\n",
    "        return _observe(state)\n",
    "\n",
    "    @property\n",
    "    def id(self) -> core.EnvId:\n",
    "        return \"minatar-freeway\"\n",
    "\n",
    "    @property\n",
    "    def version(self) -> str:\n",
    "        return \"v1\"\n",
    "\n",
    "    @property\n",
    "    def num_players(self):\n",
    "        return 1\n",
    "\n",
    "\n",
    "def _step(\n",
    "    state: State,\n",
    "    action: Array,\n",
    "    key,\n",
    "    sticky_action_prob,\n",
    "):\n",
    "    # (Kept for API completeness; not used when class-level frame_skip is active)\n",
    "    action = jnp.int32(action)\n",
    "    key0, key1 = jax.random.split(key, 2)\n",
    "    action = jax.lax.cond(\n",
    "        jax.random.uniform(key0) < sticky_action_prob,\n",
    "        lambda: state._last_action,\n",
    "        lambda: action,\n",
    "    )\n",
    "    speeds, directions = _random_speed_directions(key1)\n",
    "    return _step_det(state, action, speeds=speeds, directions=directions)\n",
    "\n",
    "\n",
    "def _init(rng: Array) -> State:\n",
    "    speeds, directions = _random_speed_directions(rng)\n",
    "    return _init_det(speeds=speeds, directions=directions)\n",
    "\n",
    "\n",
    "def _step_det(\n",
    "    state: State,\n",
    "    action: Array,\n",
    "    speeds: Array,\n",
    "    directions: Array,\n",
    "):\n",
    "    cars = state._cars\n",
    "    pos = state._pos\n",
    "    move_timer = state._move_timer\n",
    "    terminate_timer = state._terminate_timer\n",
    "    terminal = state._terminal\n",
    "    last_action = action\n",
    "\n",
    "    r = jnp.array(0, dtype=jnp.float32)\n",
    "\n",
    "    move_timer, pos = jax.lax.cond(\n",
    "        (action == 2) & (move_timer == 0),\n",
    "        lambda: (player_speed, jax.lax.max(ZERO, pos - ONE)),\n",
    "        lambda: (move_timer, pos),\n",
    "    )\n",
    "    move_timer, pos = jax.lax.cond(\n",
    "        (action == 4) & (move_timer == 0),\n",
    "        lambda: (player_speed, jax.lax.min(NINE, pos + ONE)),\n",
    "        lambda: (move_timer, pos),\n",
    "    )\n",
    "\n",
    "    # Win condition\n",
    "    cars, r, pos = jax.lax.cond(\n",
    "        pos == 0,\n",
    "        lambda: (\n",
    "            _randomize_cars(speeds, directions, cars, initialize=False),\n",
    "            r + 1,\n",
    "            NINE,\n",
    "        ),\n",
    "        lambda: (cars, r, pos),\n",
    "    )\n",
    "\n",
    "    pos, cars = _update_cars(pos, cars)\n",
    "\n",
    "    # Update various timers\n",
    "    move_timer = jax.lax.cond(\n",
    "        move_timer > 0, lambda: move_timer - 1, lambda: move_timer\n",
    "    )\n",
    "    terminate_timer -= ONE\n",
    "    terminal = terminate_timer < 0\n",
    "\n",
    "    next_state = state.replace(  # type: ignore\n",
    "        _cars=cars,\n",
    "        _pos=pos,\n",
    "        _move_timer=move_timer,\n",
    "        _terminate_timer=terminate_timer,\n",
    "        _terminal=terminal,\n",
    "        _last_action=last_action,\n",
    "        rewards=r[jnp.newaxis],\n",
    "        terminated=terminal,\n",
    "    )\n",
    "\n",
    "    return next_state\n",
    "\n",
    "\n",
    "def _update_cars(pos, cars):\n",
    "    def _update_stopped_car(pos, car):\n",
    "        car = car.at[2].set(jax.lax.abs(car[3]))\n",
    "        car = jax.lax.cond(\n",
    "            car[3] > 0, lambda: car.at[0].add(1), lambda: car.at[0].add(-1)\n",
    "        )\n",
    "        car = jax.lax.cond(car[0] < 0, lambda: car.at[0].set(9), lambda: car)\n",
    "        car = jax.lax.cond(car[0] > 9, lambda: car.at[0].set(0), lambda: car)\n",
    "        pos = jax.lax.cond(\n",
    "            (car[0] == 4) & (car[1] == pos), lambda: NINE, lambda: pos\n",
    "        )\n",
    "        return pos, car\n",
    "\n",
    "    def _update_car(pos, car):\n",
    "        pos = jax.lax.cond(\n",
    "            (car[0] == 4) & (car[1] == pos), lambda: NINE, lambda: pos\n",
    "        )\n",
    "        pos, car = jax.lax.cond(\n",
    "            car[2] == 0,\n",
    "            lambda: _update_stopped_car(pos, car),\n",
    "            lambda: (pos, car.at[2].add(-1)),\n",
    "        )\n",
    "        return pos, car\n",
    "\n",
    "    pos, cars = jax.lax.scan(_update_car, pos, cars)\n",
    "\n",
    "    return pos, cars\n",
    "\n",
    "\n",
    "def _init_det(speeds: Array, directions: Array) -> State:\n",
    "    cars = _randomize_cars(speeds, directions, initialize=True)\n",
    "    return State(_cars=cars)  # type: ignore\n",
    "\n",
    "\n",
    "def _randomize_cars(\n",
    "    speeds: Array,\n",
    "    directions: Array,\n",
    "    cars: Array = jnp.zeros((8, 4), dtype=int),\n",
    "    initialize: bool = False,\n",
    ") -> Array:\n",
    "    speeds *= directions\n",
    "\n",
    "    def _init(_cars):\n",
    "        _cars = _cars.at[:, 1].set(jnp.arange(1, 9))\n",
    "        _cars = _cars.at[:, 2].set(jax.lax.abs(speeds))\n",
    "        _cars = _cars.at[:, 3].set(speeds)\n",
    "        return _cars\n",
    "\n",
    "    def _update(_cars):\n",
    "        _cars = _cars.at[:, 2].set(abs(speeds))\n",
    "        _cars = _cars.at[:, 3].set(speeds)\n",
    "        return _cars\n",
    "\n",
    "    return jax.lax.cond(initialize, _init, _update, cars)\n",
    "\n",
    "\n",
    "def _random_speed_directions(rng):\n",
    "    rng1, rng2 = jax.random.split(rng, 2)\n",
    "    speeds = jax.random.randint(rng1, [8], 1, 6, dtype=jnp.int32)\n",
    "    directions = jax.random.choice(\n",
    "        rng2, jnp.array([-1, 1], dtype=jnp.int32), [8]\n",
    "    )\n",
    "    return speeds, directions\n",
    "\n",
    "\n",
    "def _observe(state: State) -> Array:\n",
    "    obs = jnp.zeros((10, 10, 7), dtype=jnp.bool_)\n",
    "    obs = obs.at[state._pos, 4, 0].set(TRUE)\n",
    "\n",
    "    def _update_obs(i, _obs):\n",
    "        car = state._cars[i]\n",
    "        _obs = _obs.at[car[1], car[0], 1].set(TRUE)\n",
    "        back_x = jax.lax.cond(\n",
    "            car[3] > 0, lambda: car[0] - 1, lambda: car[0] + 1\n",
    "        )\n",
    "        back_x = jax.lax.cond(back_x < 0, lambda: NINE, lambda: back_x)\n",
    "        back_x = jax.lax.cond(back_x > 9, lambda: ZERO, lambda: back_x)\n",
    "        trail = jax.lax.abs(car[3]) + 1\n",
    "        _obs = _obs.at[car[1], back_x, trail].set(TRUE)\n",
    "        return _obs\n",
    "\n",
    "    obs = jax.lax.fori_loop(0, 8, _update_obs, obs)\n",
    "    return obs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "48ba5593",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "MinAtar/Breakout with JIT-compatible K-frame skipping.\n",
    "\n",
    "Changes vs. baseline:\n",
    "- Add `frame_skip: int = 2` to MinAtarBreakout.__init__.\n",
    "- Implement frame skipping inside `_step` via `jax.lax.fori_loop`.\n",
    "- Compute a single effective action (after sticky override) and repeat it\n",
    "  for `frame_skip` internal steps, accumulating rewards. Observation comes\n",
    "  from the final internal step, and `terminated` reflects any terminal reached\n",
    "  during the repeated steps.\n",
    "\"\"\"\n",
    "\n",
    "from typing import Literal, Optional\n",
    "\n",
    "import jax\n",
    "from jax import numpy as jnp\n",
    "\n",
    "import pgx.core as core\n",
    "from pgx._src.struct import dataclass\n",
    "from pgx._src.types import Array, PRNGKey\n",
    "\n",
    "FALSE = jnp.bool_(False)\n",
    "TRUE = jnp.bool_(True)\n",
    "ZERO = jnp.array(0, dtype=jnp.int32)\n",
    "ONE = jnp.array(1, dtype=jnp.int32)\n",
    "TWO = jnp.array(2, dtype=jnp.int32)\n",
    "THREE = jnp.array(3, dtype=jnp.int32)\n",
    "FOUR = jnp.array(4, dtype=jnp.int32)\n",
    "NINE = jnp.array(9, dtype=jnp.int32)\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class State(core.State):\n",
    "    current_player: Array = jnp.int32(0)\n",
    "    observation: Array = jnp.zeros((10, 10, 4), dtype=jnp.bool_)\n",
    "    rewards: Array = jnp.zeros(1, dtype=jnp.float32)  # (1,)\n",
    "    terminated: Array = FALSE\n",
    "    truncated: Array = FALSE\n",
    "    legal_action_mask: Array = jnp.ones(3, dtype=jnp.bool_)\n",
    "    _step_count: Array = jnp.int32(0)\n",
    "    # --- MinAtar Breakout specific ---\n",
    "    _ball_y: Array = THREE\n",
    "    _ball_x: Array = ZERO\n",
    "    _ball_dir: Array = TWO\n",
    "    _pos: Array = FOUR\n",
    "    _brick_map: Array = (\n",
    "        jnp.zeros((10, 10), dtype=jnp.bool_).at[1:4, :].set(True)\n",
    "    )\n",
    "    _strike: Array = jnp.array(False, dtype=jnp.bool_)\n",
    "    _last_x: Array = ZERO\n",
    "    _last_y: Array = THREE\n",
    "    _terminal: Array = jnp.array(False, dtype=jnp.bool_)\n",
    "    _last_action: Array = ZERO\n",
    "\n",
    "    @property\n",
    "    def env_id(self) -> core.EnvId:\n",
    "        return \"minatar-breakout\"\n",
    "\n",
    "    def to_svg(\n",
    "        self,\n",
    "        *,\n",
    "        color_theme: Optional[Literal[\"light\", \"dark\"]] = None,\n",
    "        scale: Optional[float] = None,\n",
    "    ) -> str:\n",
    "        del color_theme, scale\n",
    "        from .utils import visualize_minatar\n",
    "\n",
    "        return visualize_minatar(self)\n",
    "\n",
    "    def save_svg(\n",
    "        self,\n",
    "        filename,\n",
    "        *,\n",
    "        color_theme: Optional[Literal[\"light\", \"dark\"]] = None,\n",
    "        scale: Optional[float] = None,\n",
    "    ) -> None:\n",
    "        from .utils import visualize_minatar\n",
    "\n",
    "        visualize_minatar(self, filename)\n",
    "\n",
    "\n",
    "class MinAtarBreakout(core.Env):\n",
    "    def __init__(\n",
    "        self,\n",
    "        *,\n",
    "        use_minimal_action_set: bool = True,\n",
    "        sticky_action_prob: float = 0.1,\n",
    "        frame_skip: int = 2,  # NEW: K-frame skipping (default 2)\n",
    "    ):\n",
    "        super().__init__()\n",
    "        assert frame_skip >= 1, \"frame_skip must be >= 1\"\n",
    "        self.use_minimal_action_set = use_minimal_action_set\n",
    "        self.sticky_action_prob: float = float(sticky_action_prob)\n",
    "        self.frame_skip: int = int(frame_skip)\n",
    "\n",
    "        # Minimal action set mapping (NOOP/LEFT/RIGHT for Breakout)\n",
    "        self.minimal_action_set = jnp.int32([0, 1, 3])\n",
    "\n",
    "        # Legal mask is either 6 (full) or 3 (minimal)\n",
    "        self.legal_action_mask = jnp.ones(6, dtype=jnp.bool_)\n",
    "        if self.use_minimal_action_set:\n",
    "            self.legal_action_mask = jnp.ones(\n",
    "                self.minimal_action_set.shape[0], dtype=jnp.bool_\n",
    "            )\n",
    "\n",
    "    def step(\n",
    "        self, state: core.State, action: Array, key: Optional[Array] = None\n",
    "    ) -> core.State:\n",
    "        assert key is not None, (\n",
    "            \"v2.0.0 changes the signature of step. Please specify PRNGKey at the third argument:\\n\\n\"\n",
    "            \"  * <  v2.0.0: step(state, action)\\n\"\n",
    "            \"  * >= v2.0.0: step(state, action, key)\\n\\n\"\n",
    "            \"See v2.0.0 release note for more details:\\n\\n\"\n",
    "            \"  https://github.com/sotetsuk/pgx/releases/tag/v2.0.0\"\n",
    "        )\n",
    "        return super().step(state, action, key)\n",
    "\n",
    "    def _init(self, key: PRNGKey) -> State:\n",
    "        state = _init(rng=key)  # type: ignore\n",
    "        state = state.replace(legal_action_mask=self.legal_action_mask)  # type: ignore\n",
    "        return state  # type: ignore\n",
    "\n",
    "    def _step(self, state: core.State, action, key) -> State:\n",
    "        \"\"\"One external env step = repeat the (sticky-processed) action for `frame_skip` internal steps.\"\"\"\n",
    "        # Ensure the state carries the current legal mask\n",
    "        state = state.replace(legal_action_mask=self.legal_action_mask)  # type: ignore\n",
    "\n",
    "        # Minimal action set mapping (JAX-friendly select)\n",
    "        action = jax.lax.select(\n",
    "            jnp.bool_(self.use_minimal_action_set),\n",
    "            self.minimal_action_set[action],\n",
    "            action,\n",
    "        )\n",
    "\n",
    "        # Compute effective action once (sticky override) for this macro-step\n",
    "        # Then repeat that effective action for `frame_skip` internal steps.\n",
    "        effective_action = jax.lax.cond(\n",
    "            jax.random.uniform(key) < self.sticky_action_prob,\n",
    "            lambda: jnp.int32(state._last_action),\n",
    "            lambda: jnp.int32(action),\n",
    "        )\n",
    "\n",
    "        # fori_loop carry: (State, total_reward(float32), done(bool))\n",
    "        def body_fn(i, carry):\n",
    "            s, rsum, done = carry\n",
    "\n",
    "            def do_step(args):\n",
    "                s_inner, rsum_inner, _ = args\n",
    "                s_next = _step_det(s_inner, effective_action)\n",
    "                rsum_next = rsum_inner + s_next.rewards[0]\n",
    "                done_next = s_next.terminated\n",
    "                return (s_next, rsum_next, done_next)\n",
    "\n",
    "            # If already done, keep state as-is (no-op), preserving JIT compatibility\n",
    "            return jax.lax.cond(done, lambda x: x, do_step, (s, rsum, done))\n",
    "\n",
    "        r0 = jnp.array(0.0, dtype=jnp.float32)\n",
    "        d0 = FALSE\n",
    "        state, total_r, _ = jax.lax.fori_loop(0, int(self.frame_skip), body_fn, (state, r0, d0))\n",
    "\n",
    "        # Overwrite rewards with the accumulated total for this external step\n",
    "        state = state.replace(rewards=total_r[jnp.newaxis])  # type: ignore\n",
    "        return state  # type: ignore\n",
    "\n",
    "    def _observe(self, state: core.State, player_id: Array) -> Array:\n",
    "        assert isinstance(state, State)\n",
    "        return _observe(state)\n",
    "\n",
    "    @property\n",
    "    def id(self) -> core.EnvId:\n",
    "        return \"minatar-breakout\"\n",
    "\n",
    "    @property\n",
    "    def version(self) -> str:\n",
    "        return \"v1\"\n",
    "\n",
    "    @property\n",
    "    def num_players(self):\n",
    "        return 1\n",
    "\n",
    "\n",
    "def _step_det(state: State, action: Array):\n",
    "    ball_y = state._ball_y\n",
    "    ball_x = state._ball_x\n",
    "    ball_dir = state._ball_dir\n",
    "    pos = state._pos\n",
    "    brick_map = state._brick_map\n",
    "    strike = state._strike\n",
    "    terminal = state._terminal\n",
    "\n",
    "    r = jnp.array(0, dtype=jnp.float32)\n",
    "\n",
    "    pos = _apply_action(pos, action)\n",
    "\n",
    "    # Update ball position\n",
    "    last_x = ball_x\n",
    "    last_y = ball_y\n",
    "    new_x, new_y = _update_ball_pos(ball_x, ball_y, ball_dir)\n",
    "\n",
    "    new_x, ball_dir = jax.lax.cond(\n",
    "        (new_x < 0) | (new_x > 9),\n",
    "        lambda: _update_ball_pos_x(new_x, ball_dir),\n",
    "        lambda: (new_x, ball_dir),\n",
    "    )\n",
    "\n",
    "    is_new_y_negative = new_y < 0\n",
    "    is_strike = brick_map[new_y, new_x] == 1\n",
    "    is_bottom = new_y == 9\n",
    "    new_y, ball_dir = jax.lax.cond(\n",
    "        is_new_y_negative,\n",
    "        lambda: _update_ball_pos_y(ball_dir),\n",
    "        lambda: (new_y, ball_dir),\n",
    "    )\n",
    "    strike_toggle = ~is_new_y_negative & is_strike\n",
    "    r, strike, brick_map, new_y, ball_dir = jax.lax.cond(\n",
    "        ~is_new_y_negative & is_strike & ~strike,\n",
    "        lambda: _update_by_strike(\n",
    "            r, brick_map, new_x, new_y, last_y, ball_dir, strike\n",
    "        ),\n",
    "        lambda: (r, strike, brick_map, new_y, ball_dir),\n",
    "    )\n",
    "    brick_map, new_y, ball_dir, terminal = jax.lax.cond(\n",
    "        ~is_new_y_negative & ~is_strike & is_bottom,\n",
    "        lambda: _update_by_bottom(\n",
    "            brick_map, ball_x, new_x, new_y, pos, ball_dir, last_y, terminal\n",
    "        ),\n",
    "        lambda: (brick_map, new_y, ball_dir, terminal),\n",
    "    )\n",
    "\n",
    "    strike = jax.lax.cond(\n",
    "        ~strike_toggle, lambda: jnp.zeros_like(strike), lambda: strike\n",
    "    )\n",
    "\n",
    "    state = state.replace(  # type: ignore\n",
    "        _ball_y=new_y,\n",
    "        _ball_x=new_x,\n",
    "        _ball_dir=ball_dir,\n",
    "        _pos=pos,\n",
    "        _brick_map=brick_map,\n",
    "        _strike=strike,\n",
    "        _last_x=last_x,\n",
    "        _last_y=last_y,\n",
    "        _terminal=terminal,\n",
    "        _last_action=action,\n",
    "        rewards=r[jnp.newaxis],\n",
    "        terminated=terminal,\n",
    "    )\n",
    "    return state\n",
    "\n",
    "\n",
    "def _init(rng: Array) -> State:\n",
    "    ball_start = jax.random.choice(rng, 2)\n",
    "    return _init_det(ball_start=ball_start)\n",
    "\n",
    "\n",
    "def _apply_action(pos, action):\n",
    "    pos = jax.lax.cond(\n",
    "        action == 1, lambda: jax.lax.max(ZERO, pos - ONE), lambda: pos\n",
    "    )\n",
    "    pos = jax.lax.cond(\n",
    "        action == 3, lambda: jax.lax.min(NINE, pos + ONE), lambda: pos\n",
    "    )\n",
    "    return pos\n",
    "\n",
    "\n",
    "def _update_ball_pos(ball_x, ball_y, ball_dir):\n",
    "    return jax.lax.switch(\n",
    "        ball_dir,\n",
    "        [\n",
    "            lambda: (ball_x - ONE, ball_y - ONE),\n",
    "            lambda: (ball_x + ONE, ball_y - ONE),\n",
    "            lambda: (ball_x + ONE, ball_y + ONE),\n",
    "            lambda: (ball_x - ONE, ball_y + ONE),\n",
    "        ],\n",
    "    )\n",
    "\n",
    "\n",
    "def _update_ball_pos_x(new_x, ball_dir):\n",
    "    new_x = jax.lax.max(ZERO, new_x)\n",
    "    new_x = jax.lax.min(NINE, new_x)\n",
    "    ball_dir = jnp.array([1, 0, 3, 2], dtype=jnp.int32)[ball_dir]\n",
    "    return new_x, ball_dir\n",
    "\n",
    "\n",
    "def _update_ball_pos_y(ball_dir):\n",
    "    ball_dir = jnp.array([3, 2, 1, 0], dtype=jnp.int32)[ball_dir]\n",
    "    return ZERO, ball_dir\n",
    "\n",
    "\n",
    "def _update_by_strike(r, brick_map, new_x, new_y, last_y, ball_dir, strike):\n",
    "    brick_map = brick_map.at[new_y, new_x].set(False)\n",
    "    new_y = last_y\n",
    "    ball_dir = jnp.array([3, 2, 1, 0], dtype=jnp.int32)[ball_dir]\n",
    "    return r + 1, jnp.ones_like(strike), brick_map, new_y, ball_dir\n",
    "\n",
    "\n",
    "def _update_by_bottom(\n",
    "    brick_map, ball_x, new_x, new_y, pos, ball_dir, last_y, terminal\n",
    "):\n",
    "    brick_map = jax.lax.cond(\n",
    "        brick_map.sum() == 0,\n",
    "        lambda: brick_map.at[1:4, :].set(True),\n",
    "        lambda: brick_map,\n",
    "    )\n",
    "    new_y, ball_dir, terminal = jax.lax.cond(\n",
    "        ball_x == pos,\n",
    "        lambda: (\n",
    "            last_y,\n",
    "            jnp.array([3, 2, 1, 0], dtype=jnp.int32)[ball_dir],\n",
    "            terminal,\n",
    "        ),\n",
    "        lambda: jax.lax.cond(\n",
    "            new_x == pos,\n",
    "            lambda: (\n",
    "                last_y,\n",
    "                jnp.array([2, 3, 0, 1], dtype=jnp.int32)[ball_dir],\n",
    "                terminal,\n",
    "            ),\n",
    "            lambda: (new_y, ball_dir, jnp.array(True, dtype=jnp.bool_)),\n",
    "        ),\n",
    "    )\n",
    "    return brick_map, new_y, ball_dir, terminal\n",
    "\n",
    "\n",
    "def _init_det(ball_start: Array) -> State:\n",
    "    ball_x, ball_dir = jax.lax.switch(\n",
    "        ball_start,\n",
    "        [lambda: (ZERO, TWO), lambda: (NINE, THREE)],\n",
    "    )\n",
    "    last_x = ball_x\n",
    "    return State(\n",
    "        _ball_x=ball_x, _ball_dir=ball_dir, _last_x=last_x\n",
    "    )  # type: ignore\n",
    "\n",
    "\n",
    "def _observe(state: State) -> Array:\n",
    "    obs = jnp.zeros((10, 10, 4), dtype=jnp.bool_)\n",
    "    obs = obs.at[state._ball_y, state._ball_x, 1].set(True)\n",
    "    obs = obs.at[9, state._pos, 0].set(True)\n",
    "    obs = obs.at[state._last_y, state._last_x, 2].set(True)\n",
    "    obs = obs.at[:, :, 3].set(state._brick_map)\n",
    "    return obs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87f6fdfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config: env_name='minatar-freeway' seed=0 lr=0.0003 num_envs=4096 num_eval_envs=100 num_steps=128 plan_horizon=4 total_timesteps=200000000 frame_skip=1 update_epochs=3 minibatch_size=4096 gamma=0.99 gae_lambda=0.95 clip_eps=0.2 ent_coef=0.01 vf_coef=0.5 max_grad_norm=0.5 wandb_project='pgx-minatar-ppo' save_model=True\n",
      "using custom env\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------\n",
    "# Config\n",
    "# -----------------------------\n",
    "from pydantic import BaseModel\n",
    "import pgx\n",
    "class PPOConfig(BaseModel):\n",
    "    env_name: Literal[\n",
    "        \"minatar-breakout\",\n",
    "        \"minatar-freeway\",\n",
    "        \"minatar-space_invaders\",\n",
    "        \"minatar-asterix\",\n",
    "        \"minatar-seaquest\",\n",
    "    ] = \"minatar-breakout\"\n",
    "    seed: int = 0\n",
    "    lr: float = 0.0003\n",
    "    num_envs: int = 4096\n",
    "    num_eval_envs: int = 100\n",
    "    num_steps: int = 128\n",
    "    plan_horizon: int = 4\n",
    "    total_timesteps: int = 20_000_000\n",
    "    frame_skip: int = 1\n",
    "    update_epochs: int = 3\n",
    "    minibatch_size: int = 4096\n",
    "    gamma: float = 0.99\n",
    "    gae_lambda: float = 0.95\n",
    "    clip_eps: float = 0.2\n",
    "    ent_coef: float = 0.01\n",
    "    vf_coef: float = 0.5\n",
    "    max_grad_norm: float = 0.5\n",
    "    wandb_project: str = \"pgx-minatar-ppo\"\n",
    "    save_model: bool = True\n",
    "    \n",
    "\n",
    "    class Config:\n",
    "        extra = \"forbid\"\n",
    "\n",
    "\n",
    "# In Jupyter, directly create the config instead of parsing CLI args\n",
    "# You can override any default values here\n",
    "args = PPOConfig(\n",
    "    env_name=\"minatar-freeway\",  # Change this to test different games\n",
    "    num_envs=4096,  # Smaller for testing in notebook\n",
    "    total_timesteps=200000000,  # Shorter for testing\n",
    "    frame_skip = 1,\n",
    "    save_model=True,  # Don't save in notebook by default\n",
    ")\n",
    "print(f\"Config: {args}\")\n",
    "\n",
    "env = pgx.make(str(args.env_name))\n",
    "if args.env_name == \"minatar-freeway\":\n",
    "    print(\"using custom env\")\n",
    "    env = MinAtarFreeway(\n",
    "        use_minimal_action_set=True,\n",
    "        sticky_action_prob=0.1,\n",
    "        frame_skip=args.frame_skip,\n",
    "    )\n",
    "if args.env_name == \"minatar-breakout\":\n",
    "    env = MinAtarBreakout(\n",
    "        use_minimal_action_set=True,\n",
    "        sticky_action_prob=0.1,\n",
    "        frame_skip=args.frame_skip,\n",
    "    )\n",
    "\n",
    "num_updates = args.total_timesteps // args.num_envs // args.num_steps\n",
    "num_minibatches = args.num_envs * args.num_steps // args.minibatch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "824e790e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using custom freeway env\n",
      "Starting training (PPO with expanded macro actions)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-01 20:19:04.428212: W external/xla/xla/service/gpu/autotuning/gemm_fusion_autotuner.cc:1157] Compiling 64 configs for 13 fusions on a single thread.\n",
      "2025-11-01 20:19:25.000216: W external/xla/xla/service/gpu/autotuning/dot_search_space.cc:200] All configs were filtered out because none of them sufficiently match the hints. Maybe the hints set does not contain a good representative set of valid configs?Working around this by using the full hints set instead.\n",
      "2025-11-01 20:19:25.000253: W external/xla/xla/service/gpu/autotuning/dot_search_space.cc:200] All configs were filtered out because none of them sufficiently match the hints. Maybe the hints set does not contain a good representative set of valid configs?Working around this by using the full hints set instead.\n",
      "2025-11-01 20:19:25.000276: W external/xla/xla/service/gpu/autotuning/gemm_fusion_autotuner.cc:1157] Compiling 77 configs for 4 fusions on a single thread.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'sec': 0.0, 'minatar-freeway/eval_R': 1.7400000095367432, 'steps': 0}\n",
      "{'sec': 0.7022140026092529, 'minatar-freeway/eval_R': 5.029999732971191, 'steps': 2097152}\n",
      "{'sec': 1.4007649421691895, 'minatar-freeway/eval_R': 9.239999771118164, 'steps': 4194304}\n",
      "{'sec': 2.0995163917541504, 'minatar-freeway/eval_R': 14.979999542236328, 'steps': 6291456}\n",
      "{'sec': 2.7979683876037598, 'minatar-freeway/eval_R': 18.44999885559082, 'steps': 8388608}\n",
      "{'sec': 3.4968459606170654, 'minatar-freeway/eval_R': 22.85999870300293, 'steps': 10485760}\n",
      "{'sec': 4.195537805557251, 'minatar-freeway/eval_R': 26.489999771118164, 'steps': 12582912}\n",
      "{'sec': 4.893589735031128, 'minatar-freeway/eval_R': 28.44999885559082, 'steps': 14680064}\n",
      "{'sec': 5.591802597045898, 'minatar-freeway/eval_R': 30.889999389648438, 'steps': 16777216}\n",
      "{'sec': 6.289898872375488, 'minatar-freeway/eval_R': 33.119998931884766, 'steps': 18874368}\n",
      "{'sec': 6.9886181354522705, 'minatar-freeway/eval_R': 34.45000076293945, 'steps': 20971520}\n",
      "{'sec': 7.686610460281372, 'minatar-freeway/eval_R': 36.90999984741211, 'steps': 23068672}\n",
      "{'sec': 8.384582757949829, 'minatar-freeway/eval_R': 38.87999725341797, 'steps': 25165824}\n",
      "{'sec': 9.082957983016968, 'minatar-freeway/eval_R': 41.0, 'steps': 27262976}\n",
      "{'sec': 9.780963897705078, 'minatar-freeway/eval_R': 43.52000045776367, 'steps': 29360128}\n",
      "{'sec': 10.478753328323364, 'minatar-freeway/eval_R': 45.62999725341797, 'steps': 31457280}\n",
      "{'sec': 11.177070617675781, 'minatar-freeway/eval_R': 47.45000076293945, 'steps': 33554432}\n",
      "{'sec': 11.875282526016235, 'minatar-freeway/eval_R': 49.209999084472656, 'steps': 35651584}\n",
      "{'sec': 12.581385612487793, 'minatar-freeway/eval_R': 49.91999816894531, 'steps': 37748736}\n",
      "{'sec': 13.280446529388428, 'minatar-freeway/eval_R': 52.16999816894531, 'steps': 39845888}\n",
      "{'sec': 13.97873592376709, 'minatar-freeway/eval_R': 52.78999710083008, 'steps': 41943040}\n",
      "{'sec': 14.676801204681396, 'minatar-freeway/eval_R': 53.459999084472656, 'steps': 44040192}\n",
      "{'sec': 15.374905347824097, 'minatar-freeway/eval_R': 54.38999938964844, 'steps': 46137344}\n",
      "{'sec': 16.072792291641235, 'minatar-freeway/eval_R': 55.68000030517578, 'steps': 48234496}\n",
      "{'sec': 16.7707679271698, 'minatar-freeway/eval_R': 56.279998779296875, 'steps': 50331648}\n",
      "{'sec': 17.469253301620483, 'minatar-freeway/eval_R': 56.43000030517578, 'steps': 52428800}\n",
      "{'sec': 18.16745162010193, 'minatar-freeway/eval_R': 57.5099983215332, 'steps': 54525952}\n",
      "{'sec': 18.865418195724487, 'minatar-freeway/eval_R': 57.88999938964844, 'steps': 56623104}\n",
      "{'sec': 19.56343388557434, 'minatar-freeway/eval_R': 58.07999801635742, 'steps': 58720256}\n",
      "{'sec': 20.261852741241455, 'minatar-freeway/eval_R': 58.59000015258789, 'steps': 60817408}\n",
      "{'sec': 20.959949731826782, 'minatar-freeway/eval_R': 59.05999755859375, 'steps': 62914560}\n",
      "{'sec': 21.65790581703186, 'minatar-freeway/eval_R': 59.69999694824219, 'steps': 65011712}\n",
      "{'sec': 22.366647720336914, 'minatar-freeway/eval_R': 59.62999725341797, 'steps': 67108864}\n",
      "{'sec': 23.065112113952637, 'minatar-freeway/eval_R': 59.54999923706055, 'steps': 69206016}\n",
      "{'sec': 23.763137578964233, 'minatar-freeway/eval_R': 60.279998779296875, 'steps': 71303168}\n",
      "{'sec': 24.461117267608643, 'minatar-freeway/eval_R': 60.0, 'steps': 73400320}\n",
      "{'sec': 25.160139083862305, 'minatar-freeway/eval_R': 60.94999694824219, 'steps': 75497472}\n",
      "{'sec': 25.858652114868164, 'minatar-freeway/eval_R': 60.87999725341797, 'steps': 77594624}\n",
      "{'sec': 26.556827306747437, 'minatar-freeway/eval_R': 61.06999969482422, 'steps': 79691776}\n",
      "{'sec': 27.254847526550293, 'minatar-freeway/eval_R': 61.52000045776367, 'steps': 81788928}\n",
      "{'sec': 27.952982664108276, 'minatar-freeway/eval_R': 61.84000015258789, 'steps': 83886080}\n",
      "{'sec': 28.65112018585205, 'minatar-freeway/eval_R': 62.099998474121094, 'steps': 85983232}\n",
      "{'sec': 29.35667657852173, 'minatar-freeway/eval_R': 61.459999084472656, 'steps': 88080384}\n",
      "{'sec': 30.054669618606567, 'minatar-freeway/eval_R': 61.40999984741211, 'steps': 90177536}\n",
      "{'sec': 30.753493309020996, 'minatar-freeway/eval_R': 62.18000030517578, 'steps': 92274688}\n",
      "{'sec': 31.45169162750244, 'minatar-freeway/eval_R': 62.22999954223633, 'steps': 94371840}\n",
      "{'sec': 32.149821758270264, 'minatar-freeway/eval_R': 62.189998626708984, 'steps': 96468992}\n",
      "{'sec': 32.84774136543274, 'minatar-freeway/eval_R': 62.41999816894531, 'steps': 98566144}\n",
      "{'sec': 33.545799255371094, 'minatar-freeway/eval_R': 62.94999694824219, 'steps': 100663296}\n",
      "{'sec': 34.24406027793884, 'minatar-freeway/eval_R': 62.68000030517578, 'steps': 102760448}\n",
      "{'sec': 34.9421284198761, 'minatar-freeway/eval_R': 63.369998931884766, 'steps': 104857600}\n",
      "{'sec': 35.640141010284424, 'minatar-freeway/eval_R': 62.80999755859375, 'steps': 106954752}\n",
      "{'sec': 36.338247299194336, 'minatar-freeway/eval_R': 63.05999755859375, 'steps': 109051904}\n",
      "{'sec': 37.036662340164185, 'minatar-freeway/eval_R': 63.19999694824219, 'steps': 111149056}\n",
      "{'sec': 37.73500943183899, 'minatar-freeway/eval_R': 63.69999694824219, 'steps': 113246208}\n",
      "{'sec': 38.43304371833801, 'minatar-freeway/eval_R': 63.77000045776367, 'steps': 115343360}\n",
      "{'sec': 39.13119554519653, 'minatar-freeway/eval_R': 63.69999694824219, 'steps': 117440512}\n",
      "{'sec': 39.82998251914978, 'minatar-freeway/eval_R': 64.13999938964844, 'steps': 119537664}\n",
      "{'sec': 40.528820753097534, 'minatar-freeway/eval_R': 63.72999954223633, 'steps': 121634816}\n",
      "{'sec': 41.22687602043152, 'minatar-freeway/eval_R': 64.12999725341797, 'steps': 123731968}\n",
      "{'sec': 41.92507481575012, 'minatar-freeway/eval_R': 64.01000213623047, 'steps': 125829120}\n",
      "{'sec': 42.62292504310608, 'minatar-freeway/eval_R': 63.96999740600586, 'steps': 127926272}\n",
      "{'sec': 43.32103228569031, 'minatar-freeway/eval_R': 64.0999984741211, 'steps': 130023424}\n",
      "{'sec': 44.019211292266846, 'minatar-freeway/eval_R': 64.36000061035156, 'steps': 132120576}\n",
      "{'sec': 44.71732831001282, 'minatar-freeway/eval_R': 64.5999984741211, 'steps': 134217728}\n",
      "{'sec': 45.41526532173157, 'minatar-freeway/eval_R': 64.26000213623047, 'steps': 136314880}\n",
      "{'sec': 46.1138551235199, 'minatar-freeway/eval_R': 64.90999603271484, 'steps': 138412032}\n",
      "{'sec': 46.811893939971924, 'minatar-freeway/eval_R': 63.869998931884766, 'steps': 140509184}\n",
      "{'sec': 47.51010298728943, 'minatar-freeway/eval_R': 64.75, 'steps': 142606336}\n",
      "{'sec': 48.2080717086792, 'minatar-freeway/eval_R': 64.40999603271484, 'steps': 144703488}\n",
      "{'sec': 48.90617823600769, 'minatar-freeway/eval_R': 64.79000091552734, 'steps': 146800640}\n",
      "{'sec': 49.6042845249176, 'minatar-freeway/eval_R': 65.04999542236328, 'steps': 148897792}\n",
      "{'sec': 50.302523374557495, 'minatar-freeway/eval_R': 64.81999969482422, 'steps': 150994944}\n",
      "{'sec': 51.00033664703369, 'minatar-freeway/eval_R': 65.47000122070312, 'steps': 153092096}\n",
      "{'sec': 51.69842767715454, 'minatar-freeway/eval_R': 64.75, 'steps': 155189248}\n",
      "{'sec': 52.3968300819397, 'minatar-freeway/eval_R': 64.97999572753906, 'steps': 157286400}\n",
      "{'sec': 53.095104694366455, 'minatar-freeway/eval_R': 64.76000213623047, 'steps': 159383552}\n",
      "{'sec': 53.79306125640869, 'minatar-freeway/eval_R': 64.83000183105469, 'steps': 161480704}\n",
      "{'sec': 54.51291799545288, 'minatar-freeway/eval_R': 65.0999984741211, 'steps': 163577856}\n",
      "{'sec': 55.211005210876465, 'minatar-freeway/eval_R': 64.61000061035156, 'steps': 165675008}\n",
      "{'sec': 55.91033172607422, 'minatar-freeway/eval_R': 64.97999572753906, 'steps': 167772160}\n",
      "{'sec': 56.60856103897095, 'minatar-freeway/eval_R': 64.8699951171875, 'steps': 169869312}\n",
      "{'sec': 57.306761264801025, 'minatar-freeway/eval_R': 65.19999694824219, 'steps': 171966464}\n",
      "{'sec': 58.004759073257446, 'minatar-freeway/eval_R': 65.36000061035156, 'steps': 174063616}\n",
      "{'sec': 58.70292067527771, 'minatar-freeway/eval_R': 64.91999816894531, 'steps': 176160768}\n",
      "{'sec': 59.40082097053528, 'minatar-freeway/eval_R': 64.97999572753906, 'steps': 178257920}\n",
      "{'sec': 60.09894394874573, 'minatar-freeway/eval_R': 65.33999633789062, 'steps': 180355072}\n",
      "{'sec': 60.79695677757263, 'minatar-freeway/eval_R': 65.01000213623047, 'steps': 182452224}\n",
      "{'sec': 61.495007276535034, 'minatar-freeway/eval_R': 65.41999816894531, 'steps': 184549376}\n",
      "{'sec': 62.1934335231781, 'minatar-freeway/eval_R': 65.20999908447266, 'steps': 186646528}\n",
      "{'sec': 62.89158606529236, 'minatar-freeway/eval_R': 65.5999984741211, 'steps': 188743680}\n",
      "{'sec': 63.58941602706909, 'minatar-freeway/eval_R': 65.36000061035156, 'steps': 190840832}\n",
      "{'sec': 64.28753018379211, 'minatar-freeway/eval_R': 65.5199966430664, 'steps': 192937984}\n",
      "{'sec': 64.9860429763794, 'minatar-freeway/eval_R': 65.83000183105469, 'steps': 195035136}\n",
      "{'sec': 65.68421292304993, 'minatar-freeway/eval_R': 65.81999969482422, 'steps': 197132288}\n",
      "{'sec': 66.38217878341675, 'minatar-freeway/eval_R': 65.62999725341797, 'steps': 199229440}\n",
      "Model saved to ./minatar-ppo-models/minatar-freeway//minatar-freeway-frameskip=1-N=4-macroflat.ckpt\n"
     ]
    }
   ],
   "source": [
    "# === PPO with Expanded Discrete Macro-Actions (repeat 1..N of primitive) ===\n",
    "import os, io, math, time, pickle, sys\n",
    "from functools import partial\n",
    "from typing import NamedTuple\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import optax\n",
    "from flax import nnx\n",
    "import wandb\n",
    "\n",
    "import pgx\n",
    "from pgx.experimental import auto_reset\n",
    "from pydantic import BaseModel\n",
    "from breakout_frame_skip import MinAtarBreakout\n",
    "from freeway_frame_skip import MinAtarFreeway\n",
    "\n",
    "# ---------------------------------------\n",
    "# Simple categorical wrapper\n",
    "# ---------------------------------------\n",
    "class Categorical:\n",
    "    def __init__(self, logits):\n",
    "        self.logits = logits  # [..., A]\n",
    "\n",
    "    def sample(self, seed):\n",
    "        return jax.random.categorical(seed, self.logits, axis=-1)\n",
    "\n",
    "    def log_prob(self, value):\n",
    "        log_probs = jax.nn.log_softmax(self.logits, axis=-1)\n",
    "        return jnp.take_along_axis(log_probs, value[..., None], axis=-1).squeeze(-1)\n",
    "\n",
    "    def entropy(self):\n",
    "        log_probs = jax.nn.log_softmax(self.logits, axis=-1)\n",
    "        probs = jax.nn.softmax(self.logits, axis=-1)\n",
    "        return -(probs * log_probs).sum(axis=-1)\n",
    "\n",
    "# ---------------------------------------\n",
    "# Utility\n",
    "# ---------------------------------------\n",
    "def pool_out_dim(n: int, window: int = 2, stride: int = 2, padding: str = \"VALID\") -> int:\n",
    "    if padding.upper() == \"VALID\":\n",
    "        return (n - window) // stride + 1\n",
    "    return math.ceil(n / stride)\n",
    "\n",
    "def _tree_where_batch(mask_b, new, old):\n",
    "    \"\"\"Select per-batch: where(mask_b, new, old) broadcasting mask along trailing dims.\"\"\"\n",
    "    def sel(n, o):\n",
    "        m = mask_b.reshape((mask_b.shape[0],) + (1,) * (n.ndim - 1))\n",
    "        return jnp.where(m, n, o)\n",
    "    return jax.tree.map(sel, new, old)\n",
    "\n",
    "# ---------------------------------------\n",
    "# Actor-Critic (single-step) with expanded macro-action head\n",
    "# ---------------------------------------\n",
    "class ActorCritic(nnx.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        base_num_actions: int,   # primitive actions (e.g., 3 for MinAtar minimal set)\n",
    "        obs_shape,\n",
    "        activation: str = \"tanh\",\n",
    "        *,\n",
    "        rngs: nnx.Rngs,\n",
    "        plan_horizon: int = 4,   # N (repeat choices 1..N)\n",
    "    ):\n",
    "        assert activation in [\"relu\", \"tanh\"]\n",
    "        self.base_num_actions = int(base_num_actions)\n",
    "        self.plan_horizon = int(plan_horizon)\n",
    "        self.macro_num_actions = self.base_num_actions * self.plan_horizon\n",
    "        self.activation = activation\n",
    "\n",
    "        H, W, C = obs_shape\n",
    "        # shared torso\n",
    "        self.conv = nnx.Conv(in_features=C, out_features=32, kernel_size=(2, 2), rngs=rngs)\n",
    "        self.avg_pool = partial(nnx.avg_pool, window_shape=(2, 2), strides=(2, 2), padding=\"VALID\")\n",
    "        H2 = pool_out_dim(H, 2, 2, \"VALID\")\n",
    "        W2 = pool_out_dim(W, 2, 2, \"VALID\")\n",
    "        flatten_dim = H2 * W2 * 32\n",
    "        self.fc = nnx.Linear(flatten_dim, 64, rngs=rngs)\n",
    "\n",
    "        # actor (macro head)\n",
    "        self.actor_h1 = nnx.Linear(64, 64, rngs=rngs)\n",
    "        self.actor_h2 = nnx.Linear(64, 64, rngs=rngs)\n",
    "        self.actor_out = nnx.Linear(64, self.macro_num_actions, rngs=rngs)\n",
    "\n",
    "        # critic\n",
    "        self.critic_h1 = nnx.Linear(64, 64, rngs=rngs)\n",
    "        self.critic_h2 = nnx.Linear(64, 64, rngs=rngs)\n",
    "        self.critic_out = nnx.Linear(64, 1, rngs=rngs)\n",
    "\n",
    "    def _act(self, x): \n",
    "        return nnx.relu(x) if self.activation == \"relu\" else nnx.tanh(x)\n",
    "\n",
    "    def _torso(self, x):\n",
    "        x = x.astype(jnp.float32)\n",
    "        x = self.conv(x)\n",
    "        x = nnx.relu(x)\n",
    "        x = self.avg_pool(x)\n",
    "        x = x.reshape((x.shape[0], -1))\n",
    "        x = nnx.relu(self.fc(x))\n",
    "        return x  # [B,64]\n",
    "\n",
    "    def __call__(self, x):\n",
    "        h = self._torso(x)\n",
    "        a = self._act(self.actor_h1(h)); a = self._act(self.actor_h2(a))\n",
    "        logits = self.actor_out(a)  # [B, base*A_horizon]\n",
    "        v = self._act(self.critic_h1(h)); v = self._act(self.critic_h2(v))\n",
    "        value = self.critic_out(v)  # [B,1]\n",
    "        return logits, jnp.squeeze(value, axis=-1)\n",
    "\n",
    "    # decode macro id -> (primitive in [0..base-1], repeat in [1..N])\n",
    "    def decode_macro(self, macro_id: jnp.ndarray):\n",
    "        rep_idx, prim = jnp.divmod(macro_id, self.base_num_actions)  # both [B]\n",
    "        repeat = rep_idx + 1\n",
    "        return prim.astype(jnp.int32), repeat.astype(jnp.int32)\n",
    "\n",
    "# ---------------------------------------\n",
    "# Transition container (macro as a single PPO step)\n",
    "# ---------------------------------------\n",
    "class Transition(NamedTuple):\n",
    "    done: jnp.ndarray       # [B]\n",
    "    action: jnp.ndarray     # [B]  (macro id)\n",
    "    value: jnp.ndarray      # [B]\n",
    "    reward: jnp.ndarray     # [B]  (sum with inner gamma^i)\n",
    "    log_prob: jnp.ndarray   # [B]\n",
    "    obs: jnp.ndarray        # [B,H,W,C]\n",
    "    gamma_p: jnp.ndarray    # [B]  (gamma ** repeat)\n",
    "\n",
    "# ---------------------------------------\n",
    "# Env setup (use custom env if present)\n",
    "# ---------------------------------------\n",
    "ppo_args = args  # assumes provided externally\n",
    "if ppo_args.env_name == \"minatar-breakout\" and \"MinAtarBreakout\" in globals():\n",
    "    print(\"using custom breakout env\")\n",
    "    env = MinAtarBreakout(\n",
    "        use_minimal_action_set=True,\n",
    "        sticky_action_prob=0.1,\n",
    "        frame_skip=ppo_args.frame_skip,\n",
    "    )\n",
    "elif ppo_args.env_name == \"minatar-freeway\" and \"MinAtarFreeway\" in globals():\n",
    "    print(\"using custom freeway env\")\n",
    "    env = MinAtarFreeway(\n",
    "        use_minimal_action_set=True,\n",
    "        sticky_action_prob=0.1,\n",
    "        frame_skip=ppo_args.frame_skip,\n",
    "    )\n",
    "else:\n",
    "    print(\"using default env\")\n",
    "    env = pgx.make(str(ppo_args.env_name))\n",
    "\n",
    "# ---------------------------------------\n",
    "# Optimizer\n",
    "# ---------------------------------------\n",
    "tx = optax.chain(\n",
    "    optax.clip_by_global_norm(ppo_args.max_grad_norm),\n",
    "    optax.adam(ppo_args.lr, eps=1e-5),\n",
    ")\n",
    "\n",
    "# ---------------------------------------\n",
    "# Update step (standard PPO, but each step executes a macro)\n",
    "# ---------------------------------------\n",
    "def make_update_step():\n",
    "    step_fn = jax.vmap(auto_reset(env.step, env.init))\n",
    "    base_A = int(env.num_actions)\n",
    "    N = int(ppo_args.plan_horizon)\n",
    "    gamma = jnp.float32(ppo_args.gamma)\n",
    "\n",
    "    def apply_macro(env_state, primitive: jnp.ndarray, repeats: jnp.ndarray, rng):\n",
    "        \"\"\"\n",
    "        Execute up to N external steps; only step items where i < repeats & not done_any.\n",
    "        Accumulate discounted reward inside the macro:\n",
    "          R = r_0 + gamma*r_1 + ... + gamma^(k-1)*r_{k-1}\n",
    "        Return final state, R, done_any, and gamma^k for bootstrapping.\n",
    "        \"\"\"\n",
    "        B = env_state.observation.shape[0]\n",
    "        R = jnp.zeros((B,), dtype=jnp.float32)\n",
    "        done_any = jnp.zeros((B,), dtype=jnp.bool_)\n",
    "\n",
    "        def body(i, carry):\n",
    "            state, R, done_any, rng = carry\n",
    "            active = jnp.logical_and(i < repeats, jnp.logical_not(done_any))  # [B]\n",
    "            rng, sub = jax.random.split(rng)\n",
    "            keys = jax.random.split(sub, B)\n",
    "\n",
    "            # step everyone, then select per-batch whether to accept this step\n",
    "            state_next = step_fn(state, primitive, keys)\n",
    "            r_i = jnp.squeeze(state_next.rewards, -1)  # [B]\n",
    "            term_i = state_next.terminated             # [B]\n",
    "\n",
    "            # accumulate reward only for active\n",
    "            R = R + (gamma ** i) * active.astype(jnp.float32) * r_i\n",
    "\n",
    "            # update done_any\n",
    "            done_any = jnp.logical_or(done_any, jnp.logical_and(active, term_i))\n",
    "\n",
    "            # keep or discard per batch\n",
    "            state = _tree_where_batch(active, state_next, state)\n",
    "            return (state, R, done_any, rng)\n",
    "\n",
    "        state, R, done_any, rng = jax.lax.fori_loop(0, N, body, (env_state, R, done_any, rng))\n",
    "        gamma_p = gamma ** repeats.astype(jnp.float32)  # [B]\n",
    "        return state, R, done_any, gamma_p, rng\n",
    "\n",
    "    @nnx.jit(donate_argnames=(\"model\", \"optimizer\"))\n",
    "    def _update_step(model: nnx.Module,\n",
    "                     optimizer: nnx.Optimizer,\n",
    "                     env_state,\n",
    "                     last_obs,\n",
    "                     rng):\n",
    "        # -------- Collect trajectories (num_steps macros) --------\n",
    "        def _env_step(runner_state, _):\n",
    "            model, optimizer, env_state, last_obs, rng = runner_state\n",
    "\n",
    "            # policy\n",
    "            rng, _rng = jax.random.split(rng)\n",
    "            logits, value = model(last_obs)                       # logits over macro actions\n",
    "            pi = Categorical(logits=logits)\n",
    "            macro_id = pi.sample(seed=_rng)                       # [B]\n",
    "            log_prob = pi.log_prob(macro_id)                      # [B]\n",
    "\n",
    "            # decode macro -> primitive + repeats\n",
    "            primitive, repeats = model.decode_macro(macro_id)     # both [B]\n",
    "\n",
    "            # apply macro to env\n",
    "            rng, _rng = jax.random.split(rng)\n",
    "            env_state, R_chunk, done_any, gamma_p, _ = apply_macro(env_state, primitive, repeats, _rng)\n",
    "\n",
    "            transition = Transition(\n",
    "                done_any,               # done\n",
    "                macro_id,               # action (macro id)\n",
    "                value,                  # value at obs0\n",
    "                R_chunk,                # discounted inside macro\n",
    "                log_prob,               # old logp(macro)\n",
    "                last_obs,               # obs0\n",
    "                gamma_p,                # gamma ** repeats\n",
    "            )\n",
    "            runner_state = (model, optimizer, env_state, env_state.observation, rng)\n",
    "            return runner_state, transition\n",
    "\n",
    "        runner_state = (model, optimizer, env_state, last_obs, rng)\n",
    "        runner_state, traj_batch = nnx.scan(\n",
    "            _env_step, in_axes=(nnx.Carry, None), out_axes=(nnx.Carry, 0), length=ppo_args.num_steps\n",
    "        )(runner_state, None)\n",
    "\n",
    "        # -------- Advantage / targets (GAE with per-step gamma_p) --------\n",
    "        model, optimizer, env_state, last_obs, rng = runner_state\n",
    "        _, last_val = model(last_obs)  # [B]\n",
    "\n",
    "        def _gae(carry, tr: Transition):\n",
    "            gae, next_value = carry\n",
    "            delta = tr.reward + tr.gamma_p * next_value * (1 - tr.done) - tr.value\n",
    "            gae = delta + tr.gamma_p * ppo_args.gae_lambda * (1 - tr.done) * gae\n",
    "            return (gae, tr.value), gae\n",
    "\n",
    "        (_, _), advantages = jax.lax.scan(\n",
    "            _gae, (jnp.zeros_like(last_val), last_val), traj_batch, reverse=True, unroll=16\n",
    "        )\n",
    "        targets = advantages + traj_batch.value  # [T,B]\n",
    "\n",
    "        # -------- SGD epochs (standard PPO) --------\n",
    "        def _update_epoch(update_state, _):\n",
    "            model, optimizer, traj_batch, advantages, targets, rng = update_state\n",
    "\n",
    "            def _update_minibatch(state, minibatch):\n",
    "                model, optimizer = state\n",
    "                mb_traj, mb_adv, mb_targets = minibatch\n",
    "\n",
    "                def _loss_fn(model: nnx.Module, traj: Transition, gae, targets):\n",
    "                    # re-run policy on obs0\n",
    "                    logits, value = model(traj.obs)            # logits over macro actions\n",
    "                    pi = Categorical(logits=logits)\n",
    "                    new_log_prob = pi.log_prob(traj.action)\n",
    "\n",
    "                    # value loss (clipped) at obs0\n",
    "                    value_pred_clipped = traj.value + (value - traj.value).clip(-ppo_args.clip_eps, ppo_args.clip_eps)\n",
    "                    v_loss_unclipped = jnp.square(value - targets)\n",
    "                    v_loss_clipped = jnp.square(value_pred_clipped - targets)\n",
    "                    value_loss = 0.5 * jnp.maximum(v_loss_unclipped, v_loss_clipped).mean()\n",
    "\n",
    "                    # policy loss (clipped)\n",
    "                    ratio = jnp.exp(new_log_prob - traj.log_prob)\n",
    "                    gae = (gae - gae.mean()) / (gae.std() + 1e-8)\n",
    "                    loss_actor1 = ratio * gae\n",
    "                    loss_actor2 = jnp.clip(ratio, 1.0 - ppo_args.clip_eps, 1.0 + ppo_args.clip_eps) * gae\n",
    "                    loss_actor = -jnp.minimum(loss_actor1, loss_actor2).mean()\n",
    "\n",
    "                    # entropy bonus (over macro action space)\n",
    "                    entropy = pi.entropy().mean()\n",
    "\n",
    "                    total = loss_actor + ppo_args.vf_coef * value_loss - ppo_args.ent_coef * entropy\n",
    "                    return total, (value_loss, loss_actor, entropy)\n",
    "\n",
    "                (total_loss, aux), grads = nnx.value_and_grad(\n",
    "                    _loss_fn, has_aux=True, argnums=nnx.DiffState(0, nnx.Param)\n",
    "                )(model, mb_traj, mb_adv, mb_targets)\n",
    "\n",
    "                optimizer.update(grads=grads)\n",
    "                return (model, optimizer), (total_loss, aux)\n",
    "\n",
    "            # flatten (T,B) -> (T*B)\n",
    "            rng, _rng = jax.random.split(rng)\n",
    "            batch_size = ppo_args.minibatch_size * num_minibatches\n",
    "            assert batch_size == ppo_args.num_steps * ppo_args.num_envs, \\\n",
    "                \"batch size must equal (num_steps * num_envs)\"\n",
    "\n",
    "            batch = (traj_batch, advantages, targets)\n",
    "            batch = jax.tree.map(lambda x: x.reshape((batch_size,) + x.shape[2:]), batch)\n",
    "            perm = jax.random.permutation(_rng, batch_size)\n",
    "            shuffled = jax.tree.map(lambda x: jnp.take(x, perm, axis=0), batch)\n",
    "            minibatches = jax.tree.map(\n",
    "                lambda x: jnp.reshape(x, [num_minibatches, -1] + list(x.shape[1:])),\n",
    "                shuffled,\n",
    "            )\n",
    "\n",
    "            (model, optimizer), losses = nnx.scan(\n",
    "                _update_minibatch, in_axes=(nnx.Carry, 0), out_axes=(nnx.Carry, 0)\n",
    "            )((model, optimizer), minibatches)\n",
    "            update_state = (model, optimizer, traj_batch, advantages, targets, rng)\n",
    "            return update_state, losses\n",
    "\n",
    "        update_state = (model, optimizer, traj_batch, advantages, targets, rng)\n",
    "        update_state, loss_info = nnx.scan(\n",
    "            _update_epoch, in_axes=(nnx.Carry, None), out_axes=(nnx.Carry, 0), length=ppo_args.update_epochs\n",
    "        )(update_state, None)\n",
    "\n",
    "        model, optimizer, _, _, _, rng = update_state\n",
    "        runner_state = (model, optimizer, env_state, last_obs, rng)\n",
    "        return runner_state, loss_info\n",
    "\n",
    "    return _update_step\n",
    "\n",
    "# ---------------------------------------\n",
    "# Evaluation (sample macro, execute repeats)\n",
    "# ---------------------------------------\n",
    "@nnx.jit\n",
    "def evaluate_macro(model: nnx.Module, rng_key):\n",
    "    step_fn = jax.vmap(env.step)\n",
    "    rng_key, sub_key = jax.random.split(rng_key)\n",
    "    subkeys = jax.random.split(sub_key, ppo_args.num_eval_envs)\n",
    "    state = jax.vmap(env.init)(subkeys)\n",
    "    R = jnp.zeros_like(state.rewards)  # [B,1]\n",
    "\n",
    "    def cond_fn(tup):\n",
    "        state, *_ = tup\n",
    "        return ~state.terminated.all()\n",
    "\n",
    "    def loop_macro(tup):\n",
    "        state, R, rng_key = tup\n",
    "        logits, _ = model(state.observation)\n",
    "        pi = Categorical(logits=logits)\n",
    "        rng_key, sub = jax.random.split(rng_key)\n",
    "        macro_id = pi.sample(sub)\n",
    "        # decode macro\n",
    "        base_A = env.num_actions\n",
    "        rep_idx, prim = jnp.divmod(macro_id, base_A)\n",
    "        repeats = rep_idx + 1\n",
    "\n",
    "        # execute repeats (up to N) per batch\n",
    "        def body(i, carry):\n",
    "            s, Racc, rng = carry\n",
    "            active = i < repeats\n",
    "            rng, sub2 = jax.random.split(rng)\n",
    "            keys = jax.random.split(sub2, s.observation.shape[0])\n",
    "            s_next = step_fn(s, prim, keys)\n",
    "            # select per-batch\n",
    "            s = _tree_where_batch(active, s_next, s)\n",
    "            Racc = Racc + jnp.where(active[:, None], s_next.rewards, 0.0)\n",
    "            return (s, Racc, rng)\n",
    "\n",
    "        state, R, rng_key = jax.lax.fori_loop(0, ppo_args.plan_horizon, body, (state, R, rng_key))\n",
    "        return state, R, rng_key\n",
    "\n",
    "    state, R, _ = jax.lax.while_loop(cond_fn, loop_macro, (state, R, rng_key))\n",
    "    return R.mean()\n",
    "\n",
    "# ---------------------------------------\n",
    "# Training loop (unchanged structure, macro steps collected)\n",
    "# ---------------------------------------\n",
    "def build_model(env, rng):\n",
    "    obs_shape = env.observation_shape\n",
    "    return ActorCritic(\n",
    "        env.num_actions,\n",
    "        obs_shape=obs_shape,\n",
    "        activation=\"tanh\",\n",
    "        rngs=nnx.Rngs(rng),\n",
    "        plan_horizon=ppo_args.plan_horizon,\n",
    "    )\n",
    "\n",
    "def train(rng):\n",
    "    # model + optimizer\n",
    "    rng, _rng = jax.random.split(rng)\n",
    "    model = build_model(env, _rng)\n",
    "    optimizer = nnx.Optimizer(model, tx, wrt=nnx.Param)\n",
    "\n",
    "    # update fn\n",
    "    update_step = make_update_step()\n",
    "\n",
    "    # init envs\n",
    "    rng, _rng = jax.random.split(rng)\n",
    "    reset_rng = jax.random.split(_rng, ppo_args.num_envs)\n",
    "    env_state = jax.jit(jax.vmap(env.init))(reset_rng)\n",
    "    last_obs = env_state.observation\n",
    "    rng, _rng = jax.random.split(rng)\n",
    "    runner_state = (model, optimizer, env_state, last_obs, _rng)\n",
    "\n",
    "    # warmup compile\n",
    "    _, _ = update_step(*runner_state)\n",
    "\n",
    "    # initial eval\n",
    "    steps = 0\n",
    "    rng, _rng = jax.random.split(rng)\n",
    "    eval_R = evaluate_macro(runner_state[0], _rng)\n",
    "    log = {\"sec\": 0.0, f\"{ppo_args.env_name}/eval_R\": float(eval_R), \"steps\": steps}\n",
    "    print(log)\n",
    "    if wandb.run is not None: wandb.log(log)\n",
    "    st = time.time(); tt = 0.0\n",
    "\n",
    "    for _ in range(num_updates):\n",
    "        runner_state, loss_info = update_step(*runner_state)\n",
    "        model, optimizer, env_state, last_obs, rng = runner_state\n",
    "        # rough env-steps accounting: assume avg repeat ~ plan_horizon\n",
    "        steps += ppo_args.num_envs * ppo_args.num_steps * ppo_args.plan_horizon\n",
    "\n",
    "        et = time.time(); tt += et - st\n",
    "        rng, _rng = jax.random.split(rng)\n",
    "        eval_R = evaluate_macro(model, _rng)\n",
    "        log = {\"sec\": tt, f\"{ppo_args.env_name}/eval_R\": float(eval_R), \"steps\": steps}\n",
    "        print(log)\n",
    "        if wandb.run is not None: wandb.log(log)\n",
    "        st = time.time()\n",
    "\n",
    "    return runner_state\n",
    "\n",
    "# ---------------------------------------\n",
    "# Bookkeeping, updates, and run\n",
    "# ---------------------------------------\n",
    "# If you want total_timesteps to be counted in ENV steps, this is a rough conversion\n",
    "num_updates = ppo_args.total_timesteps // (ppo_args.num_envs * ppo_args.num_steps * ppo_args.plan_horizon)\n",
    "num_minibatches = (ppo_args.num_envs * ppo_args.num_steps) // ppo_args.minibatch_size\n",
    "\n",
    "wandb.init(\n",
    "    project=f\"ppo-{ppo_args.env_name}-frameskip\",\n",
    "    name=f\"{ppo_args.env_name}-frameskip{ppo_args.frame_skip}-N{ppo_args.plan_horizon}-macroflat\",\n",
    "    config=ppo_args.dict(),\n",
    "    mode=\"disabled\",  # set \"online\" to enable logging\n",
    ")\n",
    "\n",
    "print(\"Starting training (PPO with expanded macro actions)...\")\n",
    "rng = jax.random.PRNGKey(ppo_args.seed)\n",
    "runner_state = train(rng)\n",
    "\n",
    "# save\n",
    "model_dir = f\"./minatar-ppo-models/{ppo_args.env_name}/\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "if ppo_args.save_model:\n",
    "    model = runner_state[0]\n",
    "    ckpt = f\"{model_dir}/{ppo_args.env_name}-frameskip={ppo_args.frame_skip}-N={ppo_args.plan_horizon}-macroflat.ckpt\"\n",
    "    with open(ckpt, \"wb\") as f:\n",
    "        pickle.dump(nnx.state(model, nnx.Param), f)\n",
    "    print(f\"Model saved to {ckpt}\")\n",
    "\n",
    "wandb.finish()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ed98d73",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
